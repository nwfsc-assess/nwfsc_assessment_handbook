[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unofficial NWFSC/SWFSC Groundfish Stock Assessment Handbook",
    "section": "",
    "text": "Alternative sources for best practices\nThe SSC Groundfish Sub-committee has produced documents “ACCEPTED PRACTICES GUIDELINES FOR GROUNDFISH STOCK ASSESSMENTS”. These documents are both more up-to-date than this handbook and also carry more weight. However, they are also less comprehensive than this document. You should either follow their guidelines or be prepared to justify why you didn’t.\n\n2021 version\nOlder versions (perhaps useful for understanding the choices made in previous assessments): 2019, 2017"
  },
  {
    "objectID": "01-data-sources.html",
    "href": "01-data-sources.html",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "",
    "text": "Contact info for various agencies (states, PacFIN, etc.) is listed in this Google Doc created by Jason Cope in 2009 (updated May 2021)"
  },
  {
    "objectID": "01-data-sources.html#requesting-data-from-state-partners",
    "href": "01-data-sources.html#requesting-data-from-state-partners",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.2 Requesting data from State partners",
    "text": "1.2 Requesting data from State partners\nStarting with the 2023 cycle, there is a more formal process for making data requests to states for stock assessments. Details about the data requesting process, and steps for assessment leads to take, can be found in the document “Process for requesting data from states for 2023 cycle.”"
  },
  {
    "objectID": "01-data-sources.html#regulations-and-catch-limits",
    "href": "01-data-sources.html#regulations-and-catch-limits",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.3 Regulations and catch limits",
    "text": "1.3 Regulations and catch limits\nJim Hastie has lived through many changes in regulations for west coast groundfish. Don Pearson has recently (Fall of 2014) created this online regulations database."
  },
  {
    "objectID": "01-data-sources.html#catch-data-lengths-ages-etc.",
    "href": "01-data-sources.html#catch-data-lengths-ages-etc.",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.4 Catch data, lengths, ages, etc.",
    "text": "1.4 Catch data, lengths, ages, etc.\n\n1.4.1 West Coast Groundfish Observer Program (WCGOP)\nJason Jannot says that it should be easier to request data this year, but we (or perhaps a few designated folks on the team) will get it in a more raw format that needs some processing. See section below on “Notes and best practices for observer data and discards” ADD LINK\n\n\n1.4.2 At-Sea Hake Observer Program (ASHOP)\nVanessa Tuttle (Vanessa.Tuttle@noaa.gov) is the contact person for this. A non-definitive list of “common” and “uncommon” species seen in the hake fishery (as of 2008), provided by Vanessa, is in Appendix A of this document. ADD LINK\n\n\n1.4.3 PacFIN biological data (BDS)\nTalk to John Wallace or Andi Stevens. To confirm sample sizes for BDS, go to this webpage which should give a recent tally of samples by state agency.\nAndi is developing generalized code to process the data extracted by John which is in the “PacFIN.Utilities” repository on GitHub.\nYou may see some discrepancies in CA data from 1985-1989 when compared to older extractions. Some samples were removed for some species. It is not clear why these samples were removed.\n\n\n1.4.4 PacFIN landings\n[Add text here on how and where to get catch for 2019 2023 assessments]\nTalk to John Wallace or Andi Stephens. Note that the PacFIN catch can be broken down by INPFC or month only for the trawl component, not the non-trawl gears. Also, beware double counting based on multiple levels of aggregation.\nPSMFC areas do not contain all of the catch, thus it is best to use INPFC areas to aggregate catch.\nOnce you have catch values for all years, contact the state representative(s) for confirmation that the values are correct.\n\n\n1.4.5 RecFIN\nAll states have in the past claimed that all rec. data is available on RecFIN. Users should check with state representatives for updated information on the recommended source for recreational data. One can pull historical catches here. Select the “Catch / Sample Data Reports” image which takes you to the reports dashboard. Alternatively, ODBC connections can be used to pull catch estimates directly from the Comprehensive Fish Ticket table. Some data sources (e.g. MRFSS, WA historic catch estimates) are not currently available via the public login, so you need to login with an account that has permission to view the tables. Additionally, once you have catch values for all years, contact the state representative for confirmation that the values are correct.\nTo obtain length composition data,\n\nGo to RecFIN\nSelect the “Catch / Sample Data Reports” image which takes you to the reports dashboard. There, select the “SD001 Biological Detail Report” option.\nThere is an automatic filtering applied, so to adjust select the ‘filter’ icon in the upper right (the upside down Erlenmeyer flask-like icon), and then download your data in either csv or excel format. Lengths come in imputed and measured, with T being total length and F being fork length. For questions on definitions of fields, the metadata is included as a selectable report in the reports dashboard.\n\nE.J. Dick (SWFSC) and Jason Edwards (PSMFC) are developing tables with catch-weighted length compositions. Tables have been prepared for WA and OR, and CA is in prep. Contact E.J. (edward.dick@noaa.gov) if you have questions or would like to use the estimates.\n\n\n1.4.6 Research catch\nThis is not automatically included in any of the other data sources. Gretchen Hanshew (Gretchen.Hanshew@noaa.gov) has been the source in the past. Talk to John Wallace and Ian Taylor about complexities regarding PacFIN records of landings from research catch."
  },
  {
    "objectID": "01-data-sources.html#indices-of-abundance",
    "href": "01-data-sources.html#indices-of-abundance",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.5 Indices of abundance",
    "text": "1.5 Indices of abundance\nAt a team meeting on 13 March 2019, the team agreed on the following terminology for the surveys, where best practice would be to introduce the survey initially using the full name, with the short name in parentheses, and then use the short name after that. This will be the approach used in the write-up of the surveys as well.\n\nNWFSC West Coast Groundfish Bottom Trawl Survey (WCGBT Survey)\nAFSC/NWFSC West Coast Triennial Shelf Survey (Triennial Survey)\nAFSC West Coast Slope Survey (AFSC Slope Survey)\nNWFSC West Coast Slope Survey (NWFSC Slope Survey)\nNWFSC Southern California Shelf Rockfish Hook and Line Survey (H&L Survey)\nNWFSC Integrated Acoustic and Trawl Survey of Pacific Hake (Acoustic Survey)\n\nNote: the names for the first 4 of these surveys in nwfscSurvey package are “NWFSC.Combo”, “Triennial”, “AFSC.Slope”, “NWFSC.Slope”. The H&L and acoustic surveys are not currently available through that package."
  },
  {
    "objectID": "01-data-sources.html#summary-of-noaa-fishery-independent-trawl-surveys-used-for-west-coast-assessments",
    "href": "01-data-sources.html#summary-of-noaa-fishery-independent-trawl-surveys-used-for-west-coast-assessments",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.6 Summary of NOAA fishery independent trawl surveys used for west coast assessments",
    "text": "1.6 Summary of NOAA fishery independent trawl surveys used for west coast assessments\n\nTriennial Survey (1980–1992 & 1995–2004)\n\nearly triennial (1980–1992, 55-366m, north of 36.5)\nlate triennial (1995–2004, 55-500m, north of 34.5)\nSee the 2007 Canary assessment document (Stewart, 2007) for justification of the split into two pieces, but there’s an ongoing debate about the need to split\nMust filter out water hauls and tows occurring outside the US EEZ (foreign tows)\nNote that 1977 is always tossed out.\nThe 2005 and 2013 Shortspine Thornyhead assessments split the triennial into shallow vs. deep to create a single index from 1980-2004 for 55-366 m and a separate 366-500 m index for 1995-2004.\n\nAKFSC Slope Survey (1997-2001, 183–1280m, north of 34.5)\n\nYears before 1997 surveyed small areas of the coast\nMust filter out tows occurring outside the US EEZ\n\nNWFSC Slope Survey (1998-2002, 183-1280m, north of 34.5)\nWCGBT Survey (2003-present, 55-1280m, entire US coast)\n\nStarting in 2004, there’s a change in sampling intensity north and south of 34.5, so this strata boundary should be included (unless there’s some specific reason not to)\nNote that there were changes in sampling intensity at 183 and 549 meters\nThis survey should be referenced as the “WCGBT Survey”\n\n\n\n1.6.1 AFSC surveys\nData are now available through the NWFSC data warehouse which can be accessed by the functions in the nwfscSurvey package. Appendix B identifies which year the listed vessels participated in the Triennial and Slope Surveys. Each cruise is assigned a unique number which is contained within the ‘CRUISEJOINS’ column in the database. Checking that your data has all the cruises you expect for a given survey would be good practice. Although gear and personnel change over time, if a study looking at differences by vessel was undertaken, Appendix B shows which vessels participated in what year and survey.\n\n1.6.1.1 More detail on these AFSC surveys\nThe the most recent RACE division species and data codes manuals are here:\nhttps://www.fisheries.noaa.gov/resource/document/groundfish-survey-species-code-manual-and-data-codes-manual\nThe ADP Code Book has, for example, sex and performance code information. For convenience, here is the legacy coding for sex:\n\nSex 1 Male 2 Female 3 Undetermined\n\nNote on design of the 2004 Triennial Survey\n\nIt is my (John Wallace) understanding that in the later years of Triennial survey (pre-2004), the survey became more of a fixed survey design as the skippers went back to the same locations as recorded on their vessel’s instrumentation.\nFor the 2004 survey, I followed the design as put forth in:\n2001 AFSC Triennial Survey Plan (converted from WordPerfect via Word)\nwithout regard to any previously recorded tow locations.\nFor CRUISEJOIN info, see Appendix to this document.\n\n\n\n\n1.6.2 NWFSC Survey Indices\nTech memo on “history, design, and description” of the survey is now available:\n\nhttps://www.nwfsc.noaa.gov/assets/25/8655_02272017_093722_TechMemo136.pdf\nKeller, A. A., J. R. Wallace, and R. D. Methot. 2017. The Northwest Fisheries Science Center’s West Coast Groundfish Bottom Trawl Survey: History, Design, and Description. U.S. Department of Commerce, NOAA Technical Memorandum NMFS-NWFSC-136. DOI: 10.7289/V5/TM-NWFSC-136.\n\nAdditional information on the survey can be found in these documents from John Wallace:\n\nStrata Tow Percentages for NWFSC Bottom Trawl Survey for 2004-Current\nCalcs for Strata Tow Percentages and Station Selection for the NWFSC Bottom Trawl Surveys for 2003 and Beyond (PDF file on Google Drive)\n\nIndex standardization is used Kelli Johnson’s VAST wrapper has specific examples for each survey located in the inst\\examples folder.\n\n\n1.6.3 NWFSC Survey Length and Age Compositions\nGitHub repository “nwfscSurvey” being used for comp data and other data explorations\nScaling from Tow to Stratum Level: Weight normalized length or age comps for each tow by the numerical CPUE. This is done in the standard data package we get from Beth.\nScaling from Stratum to Coastwide (or Assessment Area) level: Weight strata length or age comps by numerical index for each strata (from GLMM). This may mean dividing the biomass index for each stratum by the average weight in that stratum (likely estimated from the length comp in that stratum).\n\n1.6.3.1 Filtering recorded catches that are fish stuck in net from previous tow\nIn some cases, fish caught in one tow remain in the net until the next tow and are recorded as caught in that second tow (despite the attempts of the people on the survey to identify and exclude such fish from the data). While we plan to come up with a consistent way to deal with this, it has not been dealt with yet.\nOne problem arising from this is catch data outside of the depth range of the species. To identify the depth range, one can start with the deepest/shallowest tow with a positive record for that species and then look to see if the previous tow conducted by that vessel (likely on the same date) caught that species. If so, and if a small number or a small relative number of that fish species were recorded in the second tow, one can assume that those were from the previous tow. By moving sequentially until one reaches a depth range where a few clearly legitimate tows occurred, one can define a reasonable depth range.\nA second phase would be to define a way to filter all the tows to remove likely candidates. A simple way to do this is to look at the range of catch levels that were determined to be not legitimate, and filter the whole data set for that species by removing all tows with catches below some quantile of those catch levels (say 80 or 90%). This will eliminate having a large number of small catches in the database used for the GLMM, which would skew the modeled distribution.\n\n\n\n1.6.4 IPHC survey\nThis survey has been used for Yelloweye Rockfish and Spiny Dogfish. Claude Dykstra (claude@iphc.int) has provided data in the past. The index was calculated using a binomial GLM developed by John Wallace. Talk to John or Ian Taylor to get R code for this analysis.\n\n\n1.6.5 Oregon recreational observer program data\nTroy Buell (troy.v.buell@state.or.us) is still involved with this data source even though he has changed positions."
  },
  {
    "objectID": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards",
    "href": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.7 Notes and best practices for observer data and discards",
    "text": "1.7 Notes and best practices for observer data and discards\nDiscard mortality should be accounted for within the assessment. There are typically three ways to approach this issue.\nThe first approach is to include discard mortality into the catch data rather than modeling. Discard mortality can be included by either adding this total to the catch by fleet or by adding a discard fleet. If using the discard fleet the selectivity of this fleet will need to be mirrored (see the 2017 California Blue/Deacon assessment for an example of this approach). To obtain estimates of total discards based on the fleet structure in the model (gear and/or area) speak with Kayleigh Somers (kayleigh.somers@noaa.gov). The annual groundfish mortality report will provide detailed information in the total estimate discard by species. The GEMM report (located on the network at \\Assessments\\GEMM Report) will provide a summarized version of the discard and retained estimates from the groundfish mortality report, but have information across all WCGOP years (2002 - present). However, the groundfish mortality report (or the GEMM) will not include the most full recent year estimates (example - the GEMM available in 2019 has data through 2017) which will require an assumption regarding the most recent year’s discards. Additionally, assumptions will need to be made regarding the total amount of discard prior to the start of the WCGOP data (pre-2002).\nThe second and third alternative approaches to including discard data is to model the process of discarding within SS3. Discards can be modeled either as total discards in mt or through the rate of discarding. Both of these approaches will require discard length data to be included in the model, or if not available a specific assumption regarding the retention curve relative to the estimated selectivity curve. Estimates of total discards are available in the annual groundfish mortality report (and the GEMM). If a specific breakdown of discard by fleet is required (gear and/or area) speak with Kayleigh Somers. The data may not be able to be summarized to all fleet structures, so it is best to discuss the options with Kayleigh early in the assessment process. Assumptions regarding the discard total in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of blocks). If total discards are modeled, one will need to enter an annual CV into SS. As of right now (2019), Kayleigh is not able to provide CV quantities. This information could be calculated based on the WCGOP based on the observed data using bootstrapping. However, the bootstrap CV will be based on the observer data and may not be consistent with the data available in the groundfish mortality report (and the GEMM). Please see below for additional information regarding bootstrapping.\nModeling discard rates can be an easier approach since it does not require knowledge about the total discard amounts. However, this approach may not be feasible for stocks with limited observations in the WCGOP data due to high variability in the observed discarded and retained fish. The IFQ trawl fleet currently has near 100% observer coverage, while the non-catch share sectors have a much lower observed percentage (~20% as of last inquiry, but this may change in the future). Historically, it was thought that discard rates could not be calculated for complex managed species. However, this is not the case based on the current data. Discard rates are calculated based on the observed discarded weight relative to the total observed weight of discarded and retained fish. Bootstrapping is done using the data in order to provide a CV to the discard rate. Similar to two previous approaches, assumptions regarding the discard rate in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of blocks).\n\n1.7.1 What are the WCGOP data?\nThe WCGOP database includes only data collected by observers on West Coast vessels. The data available in this database varies fairly dramatically based on the sector. The IFQ vessels essentially have 100% observer coverage, so the database includes a comprehensive view of discards and retained catch from observers for that sector. However, other sectors have a much lower percentage of observer coverage (see the observer coverage). As an example, the nearshore fishery generally has about 20% observer coverage. Only the observations made on that percentage of vessels are going to be available in the WCGOP database. Hence, the observed retained and discard amounts are going to be a significant underestimate of total mortality in that sector. The discard ratio will also be more variable, but could generally be representative of the discarding behavior.\n\n\n1.7.2 Bootstrapping\nThe current approach to obtain uncertainty around the data in the WCGOP database, either the total discard or discard rates, is to bootstrap the data. These data are summarized and the observations bootstrapped to obtain uncertainty estimated based on gear and area stratification requested by the assessor. Chantel Wetzel (chantel.wetzel@noaa.gov) currently conducts the bootstrap analysis.\n\n\n1.7.3 Requesting discard data\nThere are two types of discard data available using WCGOP data. The first is a summary of the observed discards, discard rates, and bootstrapped uncertainty across years (2002 - present). The second type of data are lengths of discarded fish observed by WCGOP. To request these data please email information regarding data stratification (gear groupings and areas) to Chantel Wetzel (chantel.wetzel@noaa.gov) for observed discards, rates, and uncertainty and Andi Stephens (andi.stephens@noaa.gov) for length composition data for discarded fish.\n\n\n1.7.4 Discard rates, and length comps, from the Pikitch et al. Discard Study (1985-87) and Mesh Study (1988-90) databases\nIf enough data exists, discard rates and length comps from the Pikitch et al. Discard Study database (or Mesh Study if no data exists in the Discard Study) may be obtained for a species to be assessed from John Wallace. Optionally, these rates can be expanded out to the surrounding years based on applying the study rates to PacFIN catch from the expanded years. This, of course, is only reasonable when the assumption of no significant changes to the fishery is a good one. A draft of the Word document of the methods paper can be downloaded here."
  },
  {
    "objectID": "02-model-choices.html",
    "href": "02-model-choices.html",
    "title": "2  Modeling choices, notes and best practices",
    "section": "",
    "text": "The comments below are based on a team discussion on how to select a population and data maximum age for modeling:\nChoice 1, Population length bins: should extend beyond something like the 97.5% quantile of the length at age distribution for the largest age (easy to add more bins to see if it makes any difference).\nChoice 2, Max age: should be large enough for growth to be asymptotic, and at least as big as the largest data age bins. It’s easy to test the impact of changing to different value, just requires updating ageing error matrices. Look at the stable age-distribution plot and examine the number of fish in the plus group. Additionally, the period of time when data are available (after extensive exploitation), the data plus group may be lower.\nChoice 3, Data length bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. We should check to make sure that the selection of length and age bins are consistent with each other. Typically, we often have max length bins where there are only a few fish, but a larger proportion of data in the data age plus group.\nChoice 4, Data age bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. In regards to population maximum age, there is no negative repercussions within SS3 for having a larger value, beyond a slower run time. Revising the age bins based on the data, rather than apriori rules about how to set this, may be considered “data mining”. Could create a check within the survey and PacFIN comp codes that creates a flag when the data plus group has more than a certain percentage (i.e. 5%). Also, add a warning to r4ss about the percentage in the population and data plus groups."
  },
  {
    "objectID": "02-model-choices.html#determining-prior-for-natural-mortality",
    "href": "02-model-choices.html#determining-prior-for-natural-mortality",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.2 Determining prior for natural mortality",
    "text": "2.2 Determining prior for natural mortality\nOwen’s Advice on M; July 6, 2022\n\nI prefer using age data alone to estimate the natural mortality rate (see accompanying document: M2017_for_Methods_Review_Hamel; Hamel and Cope (in review)), except in cases where getting a reasonable estimate of the maximum age is problematic.\nThe age based prior is simply: \\[\\text{lognormal}(\\ln(5.4/\\text{max age}), 0.31).\\]\nThe fixed value for M if not estimating is even more simply the median of that prior: \\[M = 5.4/\\text{max age}\\]\nCan explore a range approaches for M estimates and priors here; reference is Cope and Hamel, in press."
  },
  {
    "objectID": "02-model-choices.html#estimating-ageing-error",
    "href": "02-model-choices.html#estimating-ageing-error",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.3 Estimating ageing error",
    "text": "2.3 Estimating ageing error\nJim Thorson has been working on this. These files are accessible on the web.\nYou can also find the original user manual that goes with the age error ADMB executable written by Andre Punt that is called by the above wrapper on the google drive The above user manual will help you understand what the program is doing and how to setup and run an individual file if you wish to do so."
  },
  {
    "objectID": "02-model-choices.html#maturity",
    "href": "02-model-choices.html#maturity",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.4 Maturity",
    "text": "2.4 Maturity\nTalk to Melissa Head about maturity. In 2017, she provided estimated maturity curves based on samples from the NWFSC survey for many species.\nData from the NWFSC survey on maturity includes a column indicating mature or immature and another indicating spawning and not spawning. The latter considers all “mature” fish with over 25% atresia as not spawning (along with all immature fish). The spawning/not spawning column is the one we commonly use to estimate the maturity curve since that is really what we care about. In some cases a simple logistic will fit, but if there is much skip spawning/atresia for older/larger females, a logistic type curve which asymptotes to a lower value or a non-parametric fit is more appropriate. A column with percent atresia is also provided if you with to use a percentage other than 25% for the cutoff. Finally, the mature/immature column can be used instead if the atresia/skip spawning is taken into account in specifying the fecundity relationship.\nAn additional column has been added to the NWFSC survey table indicating uncertainty in the designation. This can be used to weight or exclude data.\nNote: John Field has expressed concern that we are too focused on recent samples from the NWFSC survey, so if you aren’t going to include samples from past collections, think about a justification for that choice."
  },
  {
    "objectID": "02-model-choices.html#fecundity",
    "href": "02-model-choices.html#fecundity",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.5 Fecundity",
    "text": "2.5 Fecundity\nDick et al. 2017 (http://www.sciencedirect.com/science/article/pii/S0165783616303745) has estimates for the \\(a\\) and \\(b\\) parameters in the functional form \\(F = aL^b\\) for many rockfish. The estimates are based on length in mm and predicted number of eggs, fitted in log-space. If you use length in cm (like in SS3), and don’t want huge SSB values (eggs), you can convert the values in the paper to units of cm and millions of eggs. First, find the values “\\(\\exp(a)\\)” (referred to as \\(a'\\) below) and “\\(b\\)” from Table 6 in Dick et al. for your species (or subgenus, if no estimate for your species is reported). If your subgenus is not reported, you can use the “Sebastes” values.\nThe correct value of the “\\(a\\)” parameter (\\(F=aL^b\\)) for length in cm and fecundity in millions of eggs is:\n\\[a = \\frac{a’\\cdot10^b}{1000}\\]\nThe division is by 1 thousand instead of 1 million because recruitment in SS3 is in thousands. The value of ‘\\(b\\)’ is unchanged, and can be used directly in the assessment. The 2017 Yellowtail Rockfish assessment used units of trillions of fish by substituting 1e9 for the 1e3 in the equation above.\nThe r4ss figures in the “Bio” tab show the resulting fecundity relationship as a function of length, weight, and age so you can use that to determine whether your parameter values produce the correct final relationship."
  },
  {
    "objectID": "02-model-choices.html#modeling-selectivity",
    "href": "02-model-choices.html#modeling-selectivity",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.6 Modeling selectivity",
    "text": "2.6 Modeling selectivity\nThere was a CAPAM workshop on selectivity in 2013. Report, presentations, and recordings are available at here and the associated Fisheries Research special issue is here.\n\n2.6.1 Conceptual reasoning for using different approaches to selectivity\nThe following set of circumstances might cause selectivity to be dome-shaped:\n\nContact selectivity causing older fish to outswim the trawl, or escape the gillnet/hooks\nIncomplete spatial coverage in terms of depth or untrawlable habitat\nSpatial heterogeneity in fishing intensity (see Sampson paper). This probably applies more to fishery selectivity than surveys.\n\nReasons for justifying asymptotic selectivity\n\nIt can help estimate \\(L_\\infty\\) and variability in growth because the mode of the length comps is often representative of where the oldest fish are piling up.\nIt prevents the estimation of a large amounts of cryptic biomass\n\n\n\n2.6.2 General advice on selectivity in SS3\n\nStart with a functional form that is commonly used (double normal)\nFind some initial values using either the Excel spreadsheets or the r4ss widgets\nPut the initial values in the model and run without estimating anything until you get a model that runs. This can be done by setting the maximum phase in the starter file to 0 or (better), by using the command line inputs: -maxfn 0 -nohess.\nRead the output into R using SS_output and plot the selectivity in {r4ss} using either SS_plots(model, plot=2) or SSplotSelex(model, subplot=1), where model is the object created by SS_output().\nSet the PHASE for as many parameters to negative values as possible so that you start with estimating relatively few parameters (such as parameters 1 and 3 of the double normal, which control the peak and ascending slope).\n\n\n\n2.6.3 Guidelines for SS3 double normal initial setup\n\nFix parameters 5 and 6 (negative phase).\n\nIf selectivity is thought to be zero at the youngest/smallest or the oldest/biggest fish set the value to zero (e.g -15)\nIf selectivity is thought to be larger than zero at the youngest/smallest or the oldest/biggest fish set the value to -999 (does not scale the selectivity for the youngest or oldest age, independently from the normal curve).\n\nFix the plateau (parameter 2) to be small values (e.g. -15).\nSet the initial value for the peak (parameter 1) at the age/length equal to the mode of the composition data\nSet the ascending (parameter 3) and descending (parameter 4) slopes at \\(log(8 \\cdot (a_{peak}-a_{min}))\\) and \\(log(8 \\cdot (a_{max}-a_{peak}))\\) (substitude min and max lengths and length at peak when modeling length-base selectivity).\nDon’t estimate selectivity at youngest age/size (parameter 5) unless there are observations of fish in the smallest age- or length-bins, either fix at -5 or -999\nUse the double normal instead of the logistic for asymptotic selectivity to have flexibility of dome shape without major changes to control file. This also provides control over selectivity at the youngest age. To force a logistic shape, you can do one of the following (where parameters 2, 4, and 6 should not be estimated under any of the options):\n\nFix descending slope (parameter 4) at a large number (e.g. 15)\nAlternative 1: Fix plateau (parameter 2) to a large number (e.g. 15)\nAlternative 2: Fix selectivity of the oldest age (parameter 6) at a large number (e.g. 15)."
  },
  {
    "objectID": "02-model-choices.html#modeling-recruitment-deviations",
    "href": "02-model-choices.html#modeling-recruitment-deviations",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.7 Modeling recruitment deviations",
    "text": "2.7 Modeling recruitment deviations\n\nChoices to be made:\n\nallow recruitment deviations or not?\nrange of years?\nbreaking into “early”, “main”, “late” vectors?\n\nearly and late vectors are intended to add uncertainty to the model for years with little or no data with information about recruitment\n\n\nWhat was done in the 2011 assessments? Graphical description is here.\nGuidance on bias adjustment settings\n\nMultiple simulation analyses have shown that applying the bias adjustment settings given by the r4ss::SS_fitbiasramp() perform well on average. However, there’s no guarantee that this will work well in any given circumstance. User discretion is advised.\n\nWhat to do about \\(\\sigma_R\\).\n\nSimulation in Methot and Taylor (2011) looked at a few options.\n\nEstimating \\(\\sigma_R\\) . Performed well under ideal circumstances.\nTune \\(\\sigma_R\\) to match the observed variability in recruitment deviations. \\[ {\\sigma_R}^2 = \\text{sd}(r')^2 \\] Performed less well.\nTune \\(\\sigma_R\\) so that \\[{\\sigma_R}^2 = \\text{sd}(r')2 + \\text{mean}(\\text{SE}(r'))^2\\] where \\(\\text{sd}(r')\\) is the standard deviation of the vector of estimated recruitments over a range of years that seem reasonably well informed by the data, and \\(\\text{mean}(\\text{SE}(r'))\\) is the mean of the estimated variability of those values. Performed best.\n\n\nMCMC\n\nIf you can get MCMC to work for your model, all the worry about bias adjustment goes away. You would need to either estimate \\(\\sigma_R\\) in the MCMC and hope it converges, or fix it at a value that has been tuned to the MLE.\n\nAutocorrelated recruitment deviations\n\nEstimate # SR_autocorr (Stock-Recruit parameter in control file)\nHere are results from hake in 2019 from the ss_new file: -1 1 -0.155106 0 99 0 6 0 0 0 0 0 0 0 # SR_autocorr\nJohnson et al. (2016) estimated autocorrelation in a simulation context"
  },
  {
    "objectID": "02-model-choices.html#rockfish-steepness-profile",
    "href": "02-model-choices.html#rockfish-steepness-profile",
    "title": "2  Modeling choices, notes and best practices",
    "section": "2.8 Rockfish steepness profile",
    "text": "2.8 Rockfish steepness profile\nUp until the 2019 assessment cycle, a meta-analysis approach was used to develop a prior for steepness for West Coast rockfish species. This method was originally developed to develop this prior by Martin Dorn in 2007, and updated by Dorn in 2009/2011. It was then revised and recoded by James Thorson, who updated it in 2013/2015/2017, with Chantel Wetzel conducting the update in 2019. When the meta-analysis was updated post the 2017 assessment cycle, the estimated mean from the prior distribution was considered unreasonably high, particularly for rockfish species, and the new prior was not approved by the SSC for use in future West Coast rockfish assessments. In the instance that the SSC does not approve a new estimate (or methodology), the approved approach reverts to the previous approved estimate. In this instance that is the prior estimated for use in the 2017 assessment cycle of a mean = 0.72 with a standard deviation of 0.158.\nBelow is detailed information regarding the meta-analysis approach applied between 2007 - 2017:\n\nThe method is fully described in Thorson, Dorn, and Hamel (2018) Fisheries Research\nThe method can be replicated for any data set 2007/2009/2011/2013/2015/2017 using R package SebastesSteepness\nThe rationale for excluding a species when developing a prior for a given focal species (termed “Type-C” meta-analysis) is explained in this paper\n\nIf you are assessing any of the following species you will need to obtain a “Type-C” prior: aurora, black, bocaccio, canary, chilipepper, darkblotched, gopher, splitnose, widow, yelloweye, yellowtail north\n\nThe estimated prior by year was as follows:\n\n2007: mean = 0.58, sd = 0.181\n2009: mean = 0.69, sd = 0.218\n2011: mean = 0.76, sd = 0.170\n2013: mean = 0.78, sd = 0.152\n2015: mean = 0.77, sd = 0.147\n2017: mean = 0.72, sd = 0.158"
  },
  {
    "objectID": "03-stock-synthesis.html",
    "href": "03-stock-synthesis.html",
    "title": "3  Running Stock Synthesis",
    "section": "",
    "text": "Run the model without estimating anything at first. The best way to do this is to run as ss -stopph 0 -nohess where stopph (short for “stop phase” is equivalent to setting the maximum phase in the starter file).\nMake sure you pay attention to any notes or warnings in “warning.sso”. If you don’t understand the warnings, find out, but don’t just ignore it.\nDebugging models that don’t run. The first place to look is “echoinput.sso”. Start at the bottom and scan upwards until things start to look right or start at the top and scan downwards until things start to look wrong. It’s often obvious when you have an extra input and things model starts to go awry. Use this information to fix your input files and try again. There are some additional debugging tips in the User Manual at https://nmfs-stock-synthesis.github.io/doc/SS330_User_Manual.html#debugging-tips. Consider also reading the input files into R using r4ss::SS_read() which may help you find mismatched columns or bad values in the inputs.\nOnce the model runs, look at the “.ss_new” files. These files contain rich comments and often better formatting than your own input files. They are also good for debugging, because sometimes a model will run, but the parameter lines are associated with different fleets, or have different roles than you expected. Check the parameter names on the right hand side of “control.ss_new” to make sure everything looks right. You can then either replace your input files with the “.ss_new” files or just copy and paste elements that you want to keep. Note that if you’ve estimated any parameters, then the initial values in “control.ss_new” have been updated to these estimates.\nPull in parameter bounds that are way too wide – if you aren’t anywhere near them during minimization, extremely wide bounds (like -15 to 15 on recruit deviations, or 3 to 31 for log-R0) just slow minimization and may result in poorer convergence properties."
  },
  {
    "objectID": "03-stock-synthesis.html#model-tuning",
    "href": "03-stock-synthesis.html#model-tuning",
    "title": "3  Running Stock Synthesis",
    "section": "3.2 Model tuning",
    "text": "3.2 Model tuning\n\nIndices are typically tuned via the extra standard deviation parameter. There are many reasons to expect that the input uncertainty values on indices of abundance are underestimates of the true uncertainty. Estimating an extra uncertainty parameter has worked well in a number of west coast groundfish assessments. However, the 2021 best practices document says, “STATs should be cautious to avoid adding variability to an index as a means of resolving model structure issues such as conflicts among data sources. Rather, variability should be added to account for sampling variance underestimating index uncertainty. STATs should provide a priori reasons for why the index variability input to the model has been underestimated (or underspecified).” Note that the extra SD parameter should reflect the observed variability in survey indices rather than poor fit to the observed trend in survey indices. Resist adding SD to surveys where there are trends in residuals without evidence of hyperdepletion or hyperstability, in which case a non-linear relationship between indices and stock size is more appropriate.\nComposition data is typically tuned by either iterative Francis weighting or estimating Dirichlet-multinomial parameters. The McAllister-Ianelli method has not performed as well in simulation testing. See https://nmfs-stock-synthesis.github.io/doc/SS330_User_Manual.html#DataWeight for more info (Dirichlet-multinomial guidance was updated by Jim Thorson in September 2022).\nDiscard ratios and mean body weight These data SHOULD be tuned, but we don’t typically do so. Kelli Johnson tuned these for Sablefish in 2019 with the following description:\n\nAdded variances for discard rates and mean body weights were set using values calculated iteratively using the RMSE of differences between input and estimated values derived from SS3. Variances were parameterized in terms of standard deviation and coefficient of variation, respectively.\n\nThink about sigmaR. This could be an arbitrarily chosen value, freely estimated, or iteratively tuned. Methot and Taylor (2011) suggest a way that the tuning could be done. Whatever you choose, put a little thought into it. SigmaR should be greater than the SD of the estimated recruitment deviations. The table $sigma_R_info output by r4ss::SS_output() provides information on tuning sigmaR."
  },
  {
    "objectID": "04-post-model.html",
    "href": "04-post-model.html",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "",
    "text": "Retrospective analysis is a way to assess whether something is inconsistent in data or model assumptions. A retrospective pattern can arise from changes in catch, M, q, selectivity, or growth from that which is in the model. It is primarily an exploratory/diagnostic tool, though see point 4 below.\nSee the 11/17/2021 PEP team meeting presentation for background information and past publications on retrospective analysis\nPEP’s approach is to apply:\n\n5 years of peels (based on findings from Miller and Legault 2017),\nusing the alternative Mohn’s rho (Mohn’s rho averaged over peels - the AFSC_Hurtado term from r4ss::SSmohnsrho),\nfor depletion, biomass, fishing mortality, and recruitment. Often biomass, depletion, and recruitment are provided in assessment reports\n\n\n\n\nAlthough the east coast uses mohn’s rho to “correct” status indicators or when setting quotas, our practice is not to. This is based on past precedent, and that our fishing history is not as long as it is on the east coast.\nHurtado-Ferro et al. (2015) provide a rule of thumb on the significance of mohn’s rho (average over peels) dependent on life history. They suggest a retrospective pattern is not meaningful if \\(\\rho \\in (-0.15, 0.2)\\) for long lived species and \\(\\rho \\in (-0.22, 0.3)\\) for short lived species, and note that magnitude of Mohn’s rho not related to true bias in assessment. Miller and Legault (2017) argue that variance of Mohn’s rho is truly needed to ascertain whether an effect is significant."
  },
  {
    "objectID": "04-post-model.html#conducting-a-rebuilding-analysis-aka-puntalyzer",
    "href": "04-post-model.html#conducting-a-rebuilding-analysis-aka-puntalyzer",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "4.2 Conducting a rebuilding analysis (aka Puntalyzer)",
    "text": "4.2 Conducting a rebuilding analysis (aka Puntalyzer)\nA rebuilding plan will need to be developed for any species (or stock) that has an GFSC SSC approved stock assessment that estimates the relative spawning biomass to be below the corresponding minimum stock size threshold (0.10 for flatfish and 0.25 for all other groundfish species). A rebuilding analysis is developed using a software program called the “rebuilder” (aka the Puntalyzer) developed and maintained by Andre Punt (aepunt@uw.edu). The program is designed to work with an input rebuilding data file created by Stock Synthesis called “rebuild.dat”. The rebuilder software executable then develops numerous future projections and calculates the probability of rebuilding based on alternative harvest strategies. The RES.CSV created by the rebuilder executable contains all of the resulting estimates of rebuilding (although many quantities are not labeled within the CSV file).\nThe rebuilder github repository (pfmc-assessment/rebuilder) contains the most up-to-date rebuilder executable, the user manual, code tools for processing output, and code examples. This repository should have all the pertinent information to conduct a rebuilding analysis for a species or stock managed by the Pacific Fishery Management Council"
  },
  {
    "objectID": "04-post-model.html#r4ss",
    "href": "04-post-model.html#r4ss",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "4.3 r4ss",
    "text": "4.3 r4ss\n\nr4ss is available as an R package on CRAN but the most up-to-date version can be installed from GitHub. Installation instructions are available here.\nSee the intro vignette here for info on more advanced topics like scripting Stock Synthesis workflows with r4ss and using the tune_comps() function.\nIt’s good idea to reinstall it on a regular basis to keep up. You can also watch the github repository if you want to get notifications of changes.\nLook through all the figures that get produced by SS_plots. There have been cases where models were put forth with incorrect assumptions about biology, ageing error, etc. that could have been caught if the assessment authors had paid attention to all the plots.\nRemember that the figures can be modified to look better. For instance, you can replace the fleet names in the model with a less abbreviated set of names that will go in the plot labels.\nIf something is not working right, complain about it on the r4ss issues list."
  },
  {
    "objectID": "04-post-model.html#writing-assessment-documents",
    "href": "04-post-model.html#writing-assessment-documents",
    "title": "4  So you ran an assessment model, more notes and best practices",
    "section": "4.4 Writing assessment documents",
    "text": "4.4 Writing assessment documents\n\nRead the terms of reference and actually follow them.\nConsider using LaTeX and .Rnw files. Chantel and Kelli are working on an R package that will serve as a template for all assessment documents and act as the successor to Melissa Monk’s template. based on R Markdown.\nIf you’re using Word, talk to power users of Word about formatting of section headings, links to figures or tables, etc.. These links save much time when document is updated with sections or elements added or removed.\n\n\n\nSee the Assessment Document Template for a pre-made setup that reflects 2017-18 TORs: \\\\nwcfile\\fram\\Assessments\\Archives and shared on the Google Drive\n\n\n\nUse proper dashes and hyphens for readability: Hyphen, En dash, and Em dash\nJason says: “It would be really easy to compare results of every assessment we do with the results of the data-poor methods.” He did this for Cabezon in 2009 and thinks we could do it for all our assessments.\nFor Word, consider using Endnote or Zotero to find/organize/insert references. The assessment endnote library is at \\\\Nwcfile\\fram\\Assessment\\References, but please export references you need and do not link a Word document directly to that library.\nMake sure that labels on Figures are readable. The best way to get figures produced in R into word is to save them as PNG files. The plotting functions in the R4SS package have an input for resolution that can be increased as needed. The default resolution is higher than what you get if you save the images one by one from the RGUI graphics window.\nCrop out non-informative headings and labels from figures imported from R4SS or other packages."
  },
  {
    "objectID": "05-other.html",
    "href": "05-other.html",
    "title": "5  Other useful tidbits",
    "section": "",
    "text": "Git is a version control system that can be used to synchronize files between the web and local directories. GitHub is a user-friendly website which hosts files and depends on Git.\n\n\n\nNEEDS UPDATE INCLUDING MORE INFO ABOUT WHAT EACH PACKAGE DOES– Update: Kristin Marshall has created a Google Sheet (on 16 March 2018) to track this documentation\nSome of the sets of code used in NWFSC assessments worth noting are the following:\n\nr4ss (package: https://github.com/r4ss/r4ss)\nnwfscMapping (package: https://github.com/nwfsc-assess/nwfscMapping)\nnwfscSurvey (package: https://github.com/nwfsc-assess/nwfscSurvey)\nVAST (package: https://github.com/James-Thorson/VAST)\nAdd https://github.com/James-Thorson/SebastesSteepness\nspatialDeltaGLMM (package: https://github.com/nwfsc-assess/geostatistical_delta-GLMM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscDeltaGLM (package: https://github.com/nwfsc-assess/nwfscDeltaGLM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscAgeingError (package: https://github.com/nwfsc-assess/nwfscAgeingError)\n\nSee GitHub URL readme for R package installation instructions and example\n\nPlotting survey data with strata (from Jason):\nhttps://r-forge.r-project.org/scm/viewvc.php/JMC/?root=nwfscassmt\n[note: as of 10/28/16, plan hatched to move this file to GitHub soon]\nDiscard estimates with bootstrap uncertainty from WCGOP (https://github.com/nwfsc-assess/nwfscDiscardBootstrap)"
  },
  {
    "objectID": "appendix-a.html",
    "href": "appendix-a.html",
    "title": "6  Appendix A: Species seen* in the hake fishery",
    "section": "",
    "text": "*not a definitive list\n\n\n\n\n\n\n\n\nROCKFISH\n\nFLATFISH\n\n\n\n\n\n\n\n\n\n\nCOMMON\n\n\nCOMMON\n\n\naurora\n\n\narrowtooth flounder\n\n\nbank\n\n\ndover sole\n\n\nblackspotted\n\n\nEnglish sole\n\n\nbocaccio\n\n\nflathead sole\n\n\ncanary\n\n\nPacific halibut\n\n\nchilipepper\n\n\nPacific sanddab\n\n\ndarkblotched\n\n\npetrale sole\n\n\ngreenstriped\n\n\nrex sole\n\n\nPOP\n\n\nslender sole\n\n\nredstripe\n\n\nsouthern rock sole\n\n\nrougheye\n\n\n\n\n\n\nsharpchin\n\n\nUNCOMMON\n\n\nshortbelly\n\n\nCalifornia halibut\n\n\nshortspine\n\n\n\n\n\n\nsilvergray\n\n\n\n\n\n\nsplitnose\n\n\nSHARKS, SKATES, RAYS & RATFISH\n\n\nstripetail\n\n\n\n\n\n\nwidow\n\n\nCOMMON\n\n\nyellowtail\n\n\nbig skate\n\n\n\n\n\n\nblue shark\n\n\nUNCOMMON\n\n\nbrown cat shark\n\n\nblack\n\n\nlongnose skate\n\n\nblackgill\n\n\nPacific electric ray\n\n\nblue\n\n\nsalmon shark\n\n\nharlequin\n\n\nsandpaper skate\n\n\nlongspine\n\n\nsix gill shark\n\n\nquillback\n\n\nsoupfin shark\n\n\nredbanded\n\n\nspiny dogfish shark\n\n\nrosethorn\n\n\nspotted ratfish\n\n\nspeckled\n\n\nthresher shark\n\n\nsquarespot\n\n\n\n\n\n\ntiger\n\n\n\n\n\n\nyelloweye\n\n\nMISCELLANEOUS\n\n\nyellowmouth\n\n\n\n\n\n\n\n\n\n\nCOMMON\n\n\n\n\n\n\nAmerican shad\n\n\nSALMON\n\n\nbarracudinas\n\n\n\n\n\n\njack mackeral\n\n\nCOMMON\n\n\nking-of-the-salmon\n\n\nChinook\n\n\nlamprey\n\n\nchum\n\n\nlanternfish\n\n\ncoho\n\n\nlingcod\n\n\npink (odd years)\n\n\nPacific cod\n\n\n\n\n\n\nPacific herring\n\n\nUNCOMMON\n\n\nPacific mackeral\n\n\nsockeye\n\n\nPacific sardine\n\n\nsteelhead\n\n\npollock\n\n\n\n\n\n\nragfish\n\n\n\n\n\n\nsablefish"
  },
  {
    "objectID": "appendix-b.html",
    "href": "appendix-b.html",
    "title": "7  Appendix B: Alaska Center CRUISEJOINS for shelf and slope surveys",
    "section": "",
    "text": "TRIENNIAL SURVEY CRUISEJOINS\n\n\n\nYEAR\nCRUISEJOINS\n\n\n1977\n393 (Commando), 421 (Pacific Raider), 423 (Jordan), 500 (Tordenskjold)\n\n\n1980\n394 (Mary Lou), 404 (Pat San Marie)\n\n\n1983\n433 (Warrior II), 434 (Nordfjord)\n\n\n1986\n406 (Pat San Marie), 429 (Alaska)\n\n\n1989\n407 (Pat San Marie), 461 (Golden Fleece)\n\n\n1992\n432 (Alaska), 465 (Green Hope)\n\n\n1995\n852417 (Alaska), 852418 (Vesteraalen)\n\n\n1998\n921326 (Dominator), 929471 (Vesteraalen)\n\n\n2001\n1090096 (Sea Storm), 1090095 (Frosti)\n\n\n2004\n1236675, 1236676 Morning Star Vesteraalen\n\n\nSLOPE SURVEY CRUISEJOINS\n\n\n\n\nYEAR\nCRUISEJOINS\n\n\n1984\n435 (Half Moon Bay)\n\n\n1988\n413 (Freeman)\n\n\n1989\n462 (Golden Fleece)\n\n\n1990\n416 (Freeman)\n\n\n1991\n417 (Freeman)\n\n\n1992\n418 (Freeman)\n\n\n1993\n419 (Freeman)\n\n\n1995\n852455 (Freeman)\n\n\n1996\n869365 (Freeman)\n\n\n1997\n912730 (Freeman)\n\n\n1999\n998132 (Freeman)\n\n\n2000\n1028815 (Freeman)\n\n\n2001\n1105595 (Freeman)"
  }
]