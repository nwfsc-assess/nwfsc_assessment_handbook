# Running Stock Synthesis

## Model setup

1.  **Before running anything, read your control file into Excel.** It's too hard to check the 14 columns if they aren't lined up, and it's easy to make mistakes in these values. Make sure the phases are small positive or negative integers (and not equal to 0), that the initial values are within the bounds, and nothing else looks strange.

2.  **Run using only "ss.exe" at first, not "ss_opt.exe".** As longas you're making changes to the model, you should be using this version which checks for array bound errors. If you get such an error, work to make it go away. Once the data are set and the parameters configured, you can switch to "SS3_opt.exe" and have the model run a little faster.

3.  **Run the model without estimating anything at first.** You should do this by setting "Turn off estimation for parameters entering after this phase" to 0 in the starter file or by adding "-maxfn 0" at the command line. If you don't get a report file that looks OK at this stage, there's no point in wasting time doing any estimation. Note: the "-noest" command line input does not seem to work as well as the starter file settings, so it's probably best to avoid it.

4.  **Read the "warning.sso" file after every run** (or note the DOS index to check if warnings exist). If you don't understand the warnings, find out, but don't just ignore it.

5.  **Look at "echoinput.sso" to debug models that don't run.** This file will show you what numbers ADMB has read and how they're being interpreted by Stock Synthesis. Start at the bottom and scan upwards until things start to look right or start at the top and scan downwards until things start to look wrong. It's often obvious when you have an extra input and things model starts to go awry. Use this information to fix your input files and try again.

6.  **Once the model runs, look at the ".ss_new" files.** These files contain rich comments and often better formatting than your own input files. They are also good for debugging, because sometimes a model will run, but the parameter lines are associated with different fleets, or have different roles than you expected. Check the parameter names on the right hand side of "control.ss_new" to make sure everything looks right. You can then either replace your input files with the ".ss_new" files or just copy and paste elements that you want to keep. Note that if you've estimated any parameters, then the initial values in "control.ss_new" have been updated to these estimates.

7.  **If you have tagging data or time-varying catchability**, utilize the option to automatically generate parameter lines related to these features using the "control.ss_new" file.

8.  **Pull in parameter bounds that are way too wide** -- if you aren't anywhere near them during minimization, extremely wide bounds (like -15 to 15 on recruit deviations, or 1 to 35 for log-R0) just slow minimization and may result in poorer convergence properties.

9.  **If you have no initial equilibrium catch** in the data file for a fleet, make sure the corresponding parameter is not being estimated in the control file. Note: the equilibrium setup is different in SS3 version 3.30, so if you have any equilibrium catch, you should learn about how this setup differs than in the past.

10. **For indices of abundance,** know what you're doing. Don't mix up a CV with a standard deviation of the log. If you're using GLM output, make sure your year effects and uncertainty are transformed back into normal space (not log space or anything else).

## Model tuning

11. **Consider estimating an extra standard deviation parameter for indices.** There are many reasons to expect that the input uncertainty values on indices of abundance are underestimates of the true uncertainty. Estimating an extra uncertainty parameter has worked well in a number of west coast groundfish assessments. Note that this should reflect the observed variability in survey indices rather than poor fit to the observed trend in survey indices. i.e. resist adding SD to surveys where there are trends in residuals without evidence of hyperdepletion or hyperstability, in which case a non-linear relationship between indices and stock size is more appropriate. However, the 2021 best practices document says, "STATs should be cautious to avoid adding variability to an index as a means of resolving model structure issues such as conflicts among data sources. Rather, variability should be added to account for sampling variance underestimating index uncertainty. *STATs should provide a priori reasons for why the index variability input to the model has been underestimated (or underspecified)*."

12. **Weight the composition data.** Use one of the three methods described in the SS User Manual under "Data Weighting". The McAllister-Ianelli method used to be the most common for west coast groundfish, but now Francis and Dirichlet-Multinomial are the dominant options, with no clear reason to choose one over the other. This might change by 2021.

13. **The recruitment bias adjustment stuff is worth understanding.** See Methot and Taylor (2011) or ask Ian to explain if this is confusing. The r4ss package has a function to estimate alternative inputs for the setup of recruitments in the control file. This can make a difference in model results.

14. **Think about sigmaR.** This could be an arbitrarily chosen value, freely estimated, or iteratively tuned. Methot and Taylor (2011) suggest a way that the tuning could be done. Whatever you choose, put a little thought into it. SigmaR should be greater than the SD of the estimated recruitment deviations. See more detail on this above. There is a new (Dec. 2016) table called `$sigma_R_info` output by `r4ss::SS_output()` which provides information on tuning sigmaR.

## Estimation of uncertainty

15. See Stewart et al. (2012) or Methot and Taylor (2011) for some reasons why doing MCMC is a good thing if you have time for it.

16. Parametric bootstrap. This hasn't been done much. The [2006 hake model](https://www.pcouncil.org/documents/2006/02/stock-assessment-of-pacific-hake-whiting-in-u-s-and-canadian-waters-in-2006-february-16-2006.pdf/) included parametric bootstrap uncertainty estimates. A description is on page 41 and Figures 55-56 (pg 139-140) show some results. Doing N bootstraps requires

    a.  run model with N+2 for "Number of datafiles to produce" in starter.ss

    b.  use `r4ss::SS_splitdat()` or other tool to split apart data.ss_new

    c.  rerun model with each new data file

    d.  summarize the results, perhaps using `r4ss::SSsummarize()`

<!-- -->

17. `r4ss::SSplotPars()` can be used to compare uncertainty estimated from MLE and MCMC. `r4ss::SSplotComparisons()` works as well.

## Using MCMC

18. Talk to ~~Ian Taylor~~ Kelli Johnson or Aaron Berger if you want to run MCMC as they are using it with hake.

19. The burn-in and thinning options in the starter file are applied after whatever ADMB options are used at run time. That is, they are applied during the mceval step, not the mcmc step. You can also do burn-in or thinning in R.

20. The MCMC command automatically sets the recruitment bias adjustment settings to go to 1.0 for all years (see Methot & Taylor 2012 to see why). Therefore, the first sample of the MCMC chain probably won't match your MLE.

21. The MCMC output is in the files posteriors.sso, derived_posteriors.sso and posterior_vectors.sso. Any Report associated with an MCMC run should not be used. It may reflect the bias adjustment settings for MCMC, not the MLE. Therefore, it's safer to separate MCMC from MLE into different folders. To compare them using `r4ss::SSplotPars()`, you can merge the results into a shared folder.

22. The best available current approach to make plots from MCMC is through `r4ss::SSplotComparisons()`, which requires first reading MLE output and then adding the MCMC output. Talk to Ian or Allan about how to do this. Someday there will be a more generalized approach that can work directly with MCMC.
