[
  {
    "objectID": "01-data-sources.html",
    "href": "01-data-sources.html",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "",
    "text": "Contact info for various agencies (states, PacFIN, etc.) is listed in this Google Doc created by Jason Cope in 2009 (updated March 2018)"
  },
  {
    "objectID": "01-data-sources.html#regulations-and-catch-limits",
    "href": "01-data-sources.html#regulations-and-catch-limits",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.2 Regulations and catch limits",
    "text": "1.2 Regulations and catch limits\nJim Hastie has lived through many changes in regulations for west coast groundfish. Don Pearson has recently (Fall of 2014) created this online regulations database."
  },
  {
    "objectID": "01-data-sources.html#catch-data-lengths-ages-etc.",
    "href": "01-data-sources.html#catch-data-lengths-ages-etc.",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.3 Catch data, lengths, ages, etc.",
    "text": "1.3 Catch data, lengths, ages, etc.\n\n1.3.1 West Coast Groundfish Observer Program (WCGOP)\nJason Jannot says that it should be easier to request data this year, but we (or perhaps a few designated folks on the team) will get it in a more raw format that needs some processing. See section below on “Notes and best practices for observer data and discards” ADD LINK\n\n\n1.3.2 At-Sea Hake Observer Program (ASHOP)\nVanessa Tuttle (Vanessa.Tuttle@noaa.gov) is the contact person for this. A non-definitive list of “common” and “uncommon” species seen in the hake fishery (as of 2008), provided by Vanessa, is in Appendix A of this document. ADD LINK\n\n\n1.3.3 PacFIN biological data (BDS)\nTalk to John Wallace or Andi Stevens. To confirm sample sizes for BDS, go to this webpage which should give a recent tally of samples by state agency.\nAndi is developing generalized code to process the data extracted by John which is in the “PacFIN.Utilities” repository on GitHub.\nYou may see some discrepancies in CA data from 1985-1989 when compared to older extractions. Some samples were removed for some species. It is not clear why these samples were removed.\n\n\n1.3.4 PacFIN landings\n[Add text here on how and where to get catch for 2019 2023 assessments]\nTalk to John Wallace or Andi Stephens. Note that the PacFIN catch can be broken down by INPFC or month only for the trawl component, not the non-trawl gears. Also, beware double counting based on multiple levels of aggregation.\nPSMFC areas do not contain all of the catch, thus it is best to use INPFC areas to aggregate catch.\n\n\n1.3.5 RecFIN\nAll states have in the past claimed that all rec. data is available on RecFIN. Users should check with state representatives for updated information on the recommended source for recreational data. One can pull historical catches here. One issue that was discovered in the 2015 canary assessment was that the “Tabulate Historical Estimates (1980-2003)” tab will allow you to pull catches beyond 2003. However, the most recent catch years may not match values that can be pulled in other places on RecFIN. The best approach is to only use the “Tabulate Historical Estimates” for years 1980-2003 and pull 2004 to current from a separate location (which I cannot currently find). Additionally, once you have catch values for all years, contact the state representative for confirmation that the values are correct.\nTo obtain length composition data,\n\nGo to RecFIN\nSelect the “Catch / Sample Data Reports” image which takes you to the reports dashboard. There, select the “SD001 Biological Detail Report” option.\nThere is an automatic filtering applied, so to adjust select the ‘filter’ icon in the upper right (the upside down Erlenmeyer flask-like icon), and then download your data in either csv or excel format. Lengths come in imputed and measured, with T being total length and F being fork length. For questions on definitions of fields, the metadata is included as a selectable report in the reports dashboard.\n\n\n\n1.3.6 Research catch\nThis is not automatically included in any of the other data sources. Gretchen Hanshew (Gretchen.Hanshew@noaa.gov) has been the source in the past. Talk to John Wallace and Ian Taylor about complexities regarding PacFIN records of landings from research catch."
  },
  {
    "objectID": "01-data-sources.html#indices-of-abundance",
    "href": "01-data-sources.html#indices-of-abundance",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.4 Indices of abundance",
    "text": "1.4 Indices of abundance\nAt a team meeting on 13 March 2019, the team agreed on the following terminology for the surveys, where best practice would be to introduce the survey initially using the full name, with the short name in parentheses, and then use the short name after that. This will be the approach used in the write-up of the surveys as well.\n\nNWFSC West Coast Groundfish Bottom Trawl Survey (WCGBT Survey)\nAFSC/NWFSC West Coast Triennial Shelf Survey (Triennial Survey)\nAFSC West Coast Slope Survey (AFSC Slope Survey)\nNWFSC West Coast Slope Survey (NWFSC Slope Survey)\nNWFSC Southern California Shelf Rockfish Hook and Line Survey (H&L Survey)\nNWFSC Integrated Acoustic and Trawl Survey of Pacific Hake (Acoustic Survey)\n\nNote: the names for the first 4 of these surveys in nwfscSurvey package are “NWFSC.Combo”, “Triennial”, “AFSC.Slope”, “NWFSC.Slope”. The H&L and acoustic surveys are not currently available through that package."
  },
  {
    "objectID": "01-data-sources.html#summary-of-noaa-fishery-independent-trawl-surveys-used-for-west-coast-assessments",
    "href": "01-data-sources.html#summary-of-noaa-fishery-independent-trawl-surveys-used-for-west-coast-assessments",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.5 Summary of NOAA fishery independent trawl surveys used for west coast assessments",
    "text": "1.5 Summary of NOAA fishery independent trawl surveys used for west coast assessments\n\nTriennial Survey (1980–1992 & 1995–2004)\n\nearly triennial (1980–1992, 55-366m, north of 36.5)\nlate triennial (1995–2004, 55-500m, north of 34.5)\nSee the 2007 Canary assessment document (Stewart, 2007) for justification of the split into two pieces, but there’s an ongoing debate about the need to split\nMust filter out water hauls and tows occurring outside the US EEZ (foreign tows)\nNote that 1977 is always tossed out.\nThe 2005 and 2013 Shortspine Thornyhead assessments split the triennial into shallow vs. deep to create a single index from 1980-2004 for 55-366 m and a separate 366-500 m index for 1995-2004.\n\nAKFSC Slope Survey (1997-2001, 183–1280m, north of 34.5)\n\nYears before 1997 surveyed small areas of the coast\nMust filter out tows occurring outside the US EEZ\n\nNWFSC Slope Survey (1998-2002, 183-1280m, north of 34.5)\nWCGBT Survey (2003-present, 55-1280m, entire US coast)\n\nStarting in 2004, there’s a change in sampling intensity north and south of 34.5, so this strata boundary should be included (unless there’s some specific reason not to)\nNote that there were changes in sampling intensity at 183 and 549 meters\nThis survey should be referenced as the “WCGBT Survey”\n\n\n\n1.5.1 AFSC surveys\nData are now available through the NWFSC data warehouse which can be accessed by the functions in the nwfscSurvey package.\n\n1.5.1.1 More detail on these AFSC surveys\nThe the most recent RACE (updated yearly) ADP (data code lists) and Species Code Books are here:\nhttp://www.afsc.noaa.gov/RACE/groundfish/adp_codebook.pdf\nhttp://www.afsc.noaa.gov/RACE/groundfish/species_codebook.pdf\nThe ADP Code Book has, for example, sex and performance code information. For convenience, here’s valuable information from Page 8:\n\nSex\n‘1’= Male\n‘2’= Female\n‘3’= Undetermined\n\nNote on design of 2004 triennial survey\n\nIt is my (John Wallace) understanding that in the later years of Triennial survey (pre-2004), the survey became more of a fixed survey design as the skippers went back to the same locations as recorded on their vessel’s instrumentation.\nFor the 2004 survey, I followed the design as put forth in:\n2001 AFSC Triennial Survey Plan (converted from WordPerfect via Word)\nwithout regard to any previously recorded tow locations.\nFor CRUISEJOIN info, see Appendix to this document.\n\n\n\n\n1.5.2 NWFSC Survey Indices\nTech memo on “history, design, and description” of the survey is now available:\n\nhttps://www.nwfsc.noaa.gov/assets/25/8655_02272017_093722_TechMemo136.pdf\nKeller, A. A., J. R. Wallace, and R. D. Methot. 2017. The Northwest Fisheries Science Center’s West Coast Groundfish Bottom Trawl Survey: History, Design, and Description. U.S. Department of Commerce, NOAA Technical Memorandum NMFS-NWFSC-136. DOI: 10.7289/V5/TM-NWFSC-136.\n\nAdditional information on the survey can be found in these documents from John Wallace:\n\nStrata Tow Percentages for NWFSC Bottom Trawl Survey for 2004-Current\nCalcs for Strata Tow Percentages and Station Selection for the NWFSC Bottom Trawl Surveys for 2003 and Beyond (PDF file on Google Drive)\n\nIndex standardization is used Kelli Johnson’s VAST wrapper has specific examples for each survey located in the inst\\examples folder.\n\n\n1.5.3 NWFSC Survey Length and Age Compositions\nGitHub repository “nwfscSurvey” being used for comp data and other data explorations\nScaling from Tow to Stratum Level: Weight normalized length or age comps for each tow by the numerical CPUE. This is done in the standard data package we get from Beth.\nScaling from Stratum to Coastwide (or Assessment Area) level: Weight strata length or age comps by numerical index for each strata (from GLMM). This may mean dividing the biomass index for each stratum by the average weight in that stratum (likely estimated from the length comp in that stratum).\n\n1.5.3.1 Filtering recorded catches that are fish stuck in net from previous tow\nIn some cases, fish caught in one tow remain in the net until the next tow and are recorded as caught in that second tow (despite the attempts of the people on the survey to identify and exclude such fish from the data). While we plan to come up with a consistent way to deal with this, it has not been dealt with yet.\nOne problem arising from this is catch data outside of the depth range of the species. To identify the depth range, one can start with the deepest/shallowest tow with a positive record for that species and then look to see if the previous tow conducted by that vessel (likely on the same date) caught that species. If so, and if a small number or a small relative number of that fish species were recorded in the second tow, one can assume that those were from the previous tow. By moving sequentially until one reaches a depth range where a few clearly legitimate tows occurred, one can define a reasonable depth range.\nA second phase would be to define a way to filter all the tows to remove likely candidates. A simple way to do this is to look at the range of catch levels that were determined to be not legitimate, and filter the whole data set for that species by removing all tows with catches below some quantile of those catch levels (say 80 or 90%). This will eliminate having a large number of small catches in the database used for the GLMM, which would skew the modeled distribution.\n\n\n\n1.5.4 IPHC survey\nThis survey has been used for Yelloweye Rockfish and Spiny Dogfish. Claude Dykstra (claude@iphc.int) has provided data in the past. The index was calculated using a binomial GLM developed by John Wallace. Talk to John or Ian Taylor to get R code for this analysis.\n\n\n1.5.5 Oregon recreational observer program data\nTroy Buell (troy.v.buell@state.or.us) is still involved with this data source even though he has changed positions."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unofficial NWFSC Stock Assessment Handbook",
    "section": "",
    "text": "Alternative sources for best practices\nThe SSC Groundfish Sub-committee has produced documents \"ACCEPTED PRACTICES GUIDELINES FOR GROUNDFISH STOCK ASSESSMENTS\". These documents are both more up-to-date than this handbook and also carry more weight. However, they are also less comprehensive than this document. You should either follow their guidelines or be prepared to justify why you didn't.\n\n2021/2023 versions?\n2019 version\n2017 version"
  },
  {
    "objectID": "02-notes-best-practices.html",
    "href": "02-notes-best-practices.html",
    "title": "2  Modeling choices",
    "section": "",
    "text": "The comments below are based on a team discussion on how to select a population and data maximum age for modeling:\nChoice 1, Population length bins: should extend beyond something like the 97.5% quantile of the length at age distribution for the largest age (easy to add more bins to see if it makes any difference).\nChoice 2, Max age: should be large enough for growth to be asymptotic, and at least as big as the largest data age bins. It’s easy to test the impact of changing to different value, just requires updating ageing error matrices. Look at the stable age-distribution plot and examine the number of fish in the plus group. Additionally, the period of time when data are available (after extensive exploitation), the data plus group may be lower.\nChoice 3, Data length bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. We should check to make sure that the selection of length and age bins are consistent with each other. Typically, we often have max length bins where there are only a few fish, but a larger proportion of data in the data age plus group.\nChoice 4, Data age bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. In regards to population maximum age, there is no negative repercussions within SS3 for having a larger value, beyond a slower run time. Revising the age bins based on the data, rather than apriori rules about how to set this, may be considered “data mining”. Could create a check within the survey and PacFIN comp codes that creates a flag when the data plus group has more than a certain percentage (i.e. 5%). Also, add a warning to r4ss about the percentage in the population and data plus groups."
  },
  {
    "objectID": "02-notes-best-practices.html#what-are-the-wcgop-data",
    "href": "02-notes-best-practices.html#what-are-the-wcgop-data",
    "title": "2  Notes and best practices for observer data and discards",
    "section": "2.1 What are the WCGOP data?",
    "text": "2.1 What are the WCGOP data?\nThe WCGOP database includes only data collected by observers on West Coast vessels. The data available in this database varies fairly dramatically based on the sector. The IFQ vessels essentially have 100% observer coverage, so the database includes a comprehensive view of discards and retained catch from observers for that sector. However, other sectors have a much lower percentage of observer coverage (see the observer coverage). As an example, the nearshore fishery generally has about 20% observer coverage. Only the observations made on that percentage of vessels are going to be available in the WCGOP database. Hence, the observed retained and discard amounts are going to be a significant underestimate of total mortality in that sector. The discard ratio will also be more variable, but could generally be representative of the discarding behavior."
  },
  {
    "objectID": "02-notes-best-practices.html#bootstrapping",
    "href": "02-notes-best-practices.html#bootstrapping",
    "title": "2  Notes and best practices for observer data and discards",
    "section": "2.2 Bootstrapping",
    "text": "2.2 Bootstrapping\nThe current approach to obtain uncertainty around the data in the WCGOP database, either the total discard or discard rates, is to bootstrap the data. These data are summarized and the observations bootstrapped to obtain uncertainty estimated based on gear and area stratification requested by the assessor. Chantel Wetzel (chantel.wetzel@noaa.gov) currently conducts the bootstrap analysis."
  },
  {
    "objectID": "02-notes-best-practices.html#requesting-discard-data",
    "href": "02-notes-best-practices.html#requesting-discard-data",
    "title": "2  Notes and best practices for observer data and discards",
    "section": "2.3 Requesting discard data",
    "text": "2.3 Requesting discard data\nThere are two types of discard data available using WCGOP data. The first is a summary of the observed discards, discard rates, and bootstrapped uncertainty across years (2002 - present). The second type of data are lengths of discarded fish observed by WCGOP. To request these data please email information regarding data stratification (gear groupings and areas) to Chantel Wetzel (chantel.wetzel@noaa.gov) for observed discards, rates, and uncertainty and Andi Stephens (andi.stephens@noaa.gov) for length composition data for discarded fish."
  },
  {
    "objectID": "02-notes-best-practices.html#discard-rates-and-length-comps-from-the-pikitch-et-al.-discard-study-1985-87-and-mesh-study-1988-90-databases",
    "href": "02-notes-best-practices.html#discard-rates-and-length-comps-from-the-pikitch-et-al.-discard-study-1985-87-and-mesh-study-1988-90-databases",
    "title": "2  Notes and best practices for observer data and discards",
    "section": "2.4 Discard rates, and length comps, from the Pikitch et al. Discard Study (1985-87) and Mesh Study (1988-90) databases",
    "text": "2.4 Discard rates, and length comps, from the Pikitch et al. Discard Study (1985-87) and Mesh Study (1988-90) databases\nIf enough data exists, discard rates and length comps from the Pikitch et al. Discard Study database (or Mesh Study if no data exists in the Discard Study) may be obtained for a species to assessed from John Wallace. Optionally, these rates can be expanded out to the surrounding years based applying the study rates to PacFIN catch from the expanded years. This, of course, only works when the assumption of no significant changes to the fishery is a good one. A draft of the Word document of the methods paper (with sablefish example results soon to be updated) can be downloaded here."
  },
  {
    "objectID": "02-notes-best-practices.html#using-github",
    "href": "02-notes-best-practices.html#using-github",
    "title": "2  Notes and best practices for observer data and discards",
    "section": "3.1 Using GitHub",
    "text": "3.1 Using GitHub\nGit is a version control system that can be used to synchronize files between the web and local directories. GitHub is a user-friendly website which hosts files and depends on Git."
  },
  {
    "objectID": "02-notes-best-practices.html#shared-code-locations",
    "href": "02-notes-best-practices.html#shared-code-locations",
    "title": "2  Notes and best practices for observer data and discards",
    "section": "3.2 Shared code locations",
    "text": "3.2 Shared code locations\nNEEDS UPDATE INCLUDING MORE INFO ABOUT WHAT EACH PACKAGE DOES– Update: Kristin Marshall has created a Google Sheet (on 16 March 2018) to track this documentation\nSome of the sets of code used in NWFSC assessments worth noting are the following:\n\nr4ss (package: https://github.com/r4ss/r4ss)\nnwfscMapping (package: https://github.com/nwfsc-assess/nwfscMapping)\nnwfscSurvey (package: https://github.com/nwfsc-assess/nwfscSurvey)\nVAST (package: https://github.com/James-Thorson/VAST)\nAdd https://github.com/James-Thorson/SebastesSteepness\nspatialDeltaGLMM (package: https://github.com/nwfsc-assess/geostatistical_delta-GLMM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscDeltaGLM (package: https://github.com/nwfsc-assess/nwfscDeltaGLM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscAgeingError (package: https://github.com/nwfsc-assess/nwfscAgeingError)\n\nSee GitHub URL readme for R package installation instructions and example\n\nPlotting survey data with strata (from Jason):\nhttps://r-forge.r-project.org/scm/viewvc.php/JMC/?root=nwfscassmt\n[note: as of 10/28/16, plan hatched to move this file to GitHub soon]\nDiscard estimates with bootstrap uncertainty from WCGOP (https://github.com/nwfsc-assess/nwfscDiscardBootstrap)"
  },
  {
    "objectID": "02-notes-best-practices.html#notes-on-shared-software",
    "href": "02-notes-best-practices.html#notes-on-shared-software",
    "title": "2  Notes and best practices",
    "section": "2.2 Notes on shared software",
    "text": "2.2 Notes on shared software\n\n2.2.1 Using GitHub\nGit is a version control system that can be used to synchronize files between the web and local directories. GitHub is a user-friendly website which hosts files and depends on Git.\n\n\n2.2.2 Shared code locations\nNEEDS UPDATE INCLUDING MORE INFO ABOUT WHAT EACH PACKAGE DOES– Update: Kristin Marshall has created a Google Sheet (on 16 March 2018) to track this documentation\nSome of the sets of code used in NWFSC assessments worth noting are the following:\n\nr4ss (package: https://github.com/r4ss/r4ss)\nnwfscMapping (package: https://github.com/nwfsc-assess/nwfscMapping)\nnwfscSurvey (package: https://github.com/nwfsc-assess/nwfscSurvey)\nVAST (package: https://github.com/James-Thorson/VAST)\nAdd https://github.com/James-Thorson/SebastesSteepness\nspatialDeltaGLMM (package: https://github.com/nwfsc-assess/geostatistical_delta-GLMM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscDeltaGLM (package: https://github.com/nwfsc-assess/nwfscDeltaGLM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscAgeingError (package: https://github.com/nwfsc-assess/nwfscAgeingError)\n\nSee GitHub URL readme for R package installation instructions and example\n\nPlotting survey data with strata (from Jason):\nhttps://r-forge.r-project.org/scm/viewvc.php/JMC/?root=nwfscassmt\n[note: as of 10/28/16, plan hatched to move this file to GitHub soon]\nDiscard estimates with bootstrap uncertainty from WCGOP (https://github.com/nwfsc-assess/nwfscDiscardBootstrap)"
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-determining-prior-for-natural-mortality",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-determining-prior-for-natural-mortality",
    "title": "2  Modeling choices",
    "section": "2.2 Notes and best practices for determining prior for natural mortality",
    "text": "2.2 Notes and best practices for determining prior for natural mortality\nOwen’s Advice on M; July 6, 2022\n\nI prefer using age data alone to estimate the natural mortality rate (see accompanying document: M2017_for_Methods_Review_Hamel; Hamel and Cope (in review)), except in cases where getting a reasonable estimate of the maximum age is problematic.\nThe age based prior is simply: \\[\\text{lognormal}(\\ln(5.4/\\text{max age}), 0.31).\\]\nThe fixed value for M if not estimating is even more simply the median of that prior: \\[M = 5.4/\\text{max age}\\]\nCan explore a range approaches for M estimates and priors here; reference is Cope and Hamel, in press."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-of-estimating-aging-error",
    "href": "02-notes-best-practices.html#notes-and-best-practices-of-estimating-aging-error",
    "title": "2  Notes and best practices",
    "section": "3.2 Notes and best practices of estimating aging error",
    "text": "3.2 Notes and best practices of estimating aging error\nJim Thorson has been working on this. These files are accessible on the web.\nYou can also find the original user manual that goes with the age error ADMB executable written by Andre Punt that is called by the above wrapper on the google drive The above user manual will help you understand what the program is doing and how to setup and run an individual file if you wish to do so."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-maturity",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-maturity",
    "title": "2  Modeling choices",
    "section": "2.4 Notes and best practices for maturity",
    "text": "2.4 Notes and best practices for maturity\nTalk to Melissa Head about maturity. In 2017, she provided estimated maturity curves based on samples from the NWFSC survey for many species.\nData from the NWFSC survey on maturity includes a column indicating mature or immature and another indicating spawning and not spawning. The latter considers all “mature” fish with over 25% atresia as not spawning (along with all immature fish). The spawning/not spawning column is the one we commonly use to estimate the maturity curve since that is really what we care about. In some cases a simple logistic will fit, but if there is much skip spawning/atresia for older/larger females, a logistic type curve which asymptotes to a lower value or a non-parametric fit is more appropriate. A column with percent atresia is also provided if you with to use a percentage other than 25% for the cutoff. Finally, the mature/immature column can be used instead if the atresia/skip spawning is taken into account in specifying the fecundity relationship.\nAn additional column has been added to the NWFSC survey table indicating uncertainty in the designation. This can be used to weight or exclude data.\nNote: John Field has expressed concern that we are too focused on recent samples from the NWFSC survey, so if you aren’t going to include samples from past collections, think about a justification for that choice."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-fecundity",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-fecundity",
    "title": "2  Modeling choices",
    "section": "2.5 Notes and best practices for fecundity",
    "text": "2.5 Notes and best practices for fecundity\nUpdate (6/05/17): EJ published a new analysis in 2017 which has estimates for \\(aL^b\\) where L is in cm.\nIan Taylor to add his notes on how to set this up in SS3 here soon.\nUpdate (11/29/16):\nAnalysis of fecundity with size can be found for many rockfishes in E.J. Dick’s Dissertation. The reference (from 2013 darkblotched document) is “Dick, E. J. 2009. Modeling the reproductive potential of rockfishes (Sebastes spp.). Ph.D. Dissertation, University of California, Santa Cruz.”\nChapter 3 of E.J.’s dissertation describes his methods. Tables 10 and 11 list parameters (for each species that E.J. considered), which include the intercept and the slope in the 2-level hierarchical model for relative fecundity as a linear function of weight. The intercept is a scalar (eggs/kg at ‘zero’ weight), and the slope is the expected increase in weight-specific fecundity per additional kg of weight. Note that you want to use the median (50%) value from those tables.\nThe units of the response variable for SS3 fecundity option #1 are eggs/kg. E.J.’s regression for SS3 option #1 was in grams; therefore, we need to multiply the intercept by 1,000 and the slope by 1,000,000 to convert to kg (since that model is quadratic in W).\nSo using darkblotched as an example, for fecundity option #1 in SS3, the parameters to enter into the control file are: alpha = 101,100 and beta = 44,800\nFor another example see the control file in the 2009 or 2011 yelloweye assessments and for Ian S.’s text see section 2.2.3 in the 2009 yelloweye assessment. Note that the comment in the control file has ‘eggs/cm’ however the conversion is made and this should be ‘eggs/kg’\nHere is bit of R code from John Wallace to see that this is the correct approach:\nx.gm <- seq(1, 10, length = 2000)\ny.eggs.per.gm  <- rnorm(2000, 2 + 3*x.gm)\nplot(x.gm, y.eggs.per.gm)\nsummary(glm(y.eggs.per.gm ~ x.gm))\n\nx.kg <- x.gm/1000\ny.eggs.per.kg  <- y.eggs.per.gm * 1000\nplot(x.kg, y.eggs.per.kg)\nsummary(glm(y.eggs.per.kg ~ x.kg))\nThe r4ss figure bio6_fecundity.png shows the resulting relationship so you can use that to determine whether your parameter values produce the correct final relationship."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-modeling-selectivity",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-modeling-selectivity",
    "title": "2  Modeling choices",
    "section": "2.6 Notes and best practices for modeling selectivity",
    "text": "2.6 Notes and best practices for modeling selectivity\nIan Taylor and Jim Thorson will fill in some notes here based on the CAPAM selectivity workshop held in La Jolla on March 11-14, 2013. The notes below are a start suggested by Mark Maunder.\n[update: as of July 2, 2014, Ian and Jim haven’t added anything to the text below but the Special Issue of Fisheries Science associated with the CAPAM meeting isn’t yet publicly available (it’s close)]\n\n2.6.1 Conceptual reasoning for using different approaches to selectivity\nThe following set of circumstances might cause selectivity to be dome-shaped:\n\nContact selectivity causing older fish to outswim the trawl, or escape the gillnet/hooks\nIncomplete spatial coverage in terms of depth or untrawlable habitat\nSpatial heterogeneity in fishing intensity (see Sampson paper). This probably applies more to fishery selectivity than surveys.\n\nReasons for justifying asymptotic selectivity\n\n\n\n\n\n2.6.2 General advice on selectivity in SS3\n\nStart with a functional form that is commonly used (double normal)\nFind some initial values using either the Excel spreadsheets or the r4ss widgets\nPut the initial values in the model and run without estimating anything until you get a model that runs. This can be done by setting the maximum phase in the starter file to 0 or (better), by using the command line inputs: “-maxfn 0 -nohess”\nRead the output into R using SS_output and plot the selectivity using either SS_plots(…, plot=2) or SSplotSelex(…,subplot=1), where … is the object created by SS_output.\nSet the PHASE for as many parameters to negative values as possible so that you start with estimating relatively few parameters (such as parameters 1 and 3 of the double normal, which control the peak and ascending slope).\n\n\n\n2.6.3 Guidelines for SS3 double normal initial setup\n\nFix parameters 5 and 6 (negative phase).\n\nIf selectivity is thought to be zero at the youngest/smallest or the oldest/biggest fish set the value to zero (e.g -15)\nIf selectivity is thought to be larger than zero at the youngest/smallest or the oldest/biggest fish set the value to -999 (does not scale the selectivity for the youngest or oldest age, independently from the normal curve).\n\nFix the plateau (parameter 2) to be small values (e.g. -15).\nSet the peak (parameter 1) at the age/length equal to the mode of the composition data\nSet the ascending (parameter 3) and descending (parameter 4) slopes at ln( (peak_age-youngest_age)8) and ln( (oldestyoungest_age-peak_age)8)\nDon’t estimate selectivity at youngest age/size (parameter 5), either fix at -5 or -999\nUse the double normal for asymptotic selectivity so have flexibility of dome shape without major changes to control file, plus have control over selectivity at the youngest age.\n\nFix descending slope (parameter 4) at a large number (e.g. 15)\nAlternative 1: Fix plateau (parameter 2) to a large number (e.g. 15)\nAlternative 2: Fix selectivity of the oldest age (parameter 6) at a large number (e.g. 15)."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-modeling-recruitment-deviations",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-modeling-recruitment-deviations",
    "title": "2  Modeling choices",
    "section": "2.7 Notes and best practices for modeling recruitment deviations",
    "text": "2.7 Notes and best practices for modeling recruitment deviations\n\nChoices to be made:\n\nallow recruitment deviations or not?\nrange of years?\nbreaking into “early”, “main”, “late” vectors?\n\nearly and late vectors are intended to add uncertainty to the model for years with little or no data with information about recruitment\n\n\nWhat was done in the 2011 assessments? Graphical description is here.\nGuidance on bias adjustment settings\n\nMultiple simulation analyses have shown that applying the bias adjustment settings given by the r4ss::SS_fitbiasramp() perform well on average. However, there’s no guarantee that this will work well in any given circumstance. User discretion is advised.\n\nWhat to do about \\(\\sigma_R\\).\n\nSimulation in Methot and Taylor (2011) looked at a few options.\n\nEstimating \\(\\sigma_R\\) . Performed well under ideal circumstances.\nTune \\(\\sigma_R\\) to match the observed variability in recruitment deviations. \\[ {\\sigma_R}^2 = \\text{sd}(r')^2 \\] Performed less well.\nTune \\(\\sigma_R\\) so that \\[{\\sigma_R}^2 = \\text{sd}(r')2 + \\text{mean}(\\text{SE}(r'))^2\\] where \\(\\text{sd}(r')\\) is the standard deviation of the vector of estimated recruitments over a range of years that seem reasonably well informed by the data, and \\(\\text{mean}(\\text{SE}(r'))\\) is the mean of the estimated variability of those values. Performed best.\n\n\nMCMC\n\nIf you can get MCMC to work for your model, all the worry about bias adjustment goes away. You would need to either estimate \\(\\sigma_R\\) in the MCMC and hope it converges, or fix it at a value that has been tuned to the MLE.\n\nAutocorrelated recruitment deviations\n\nEstimate # SR_autocorr (Stock-Recruit parameter in control file)\nHere are results from hake in 2019 from the ss_new file: -1 1 -0.155106 0 99 0 6 0 0 0 0 0 0 0 # SR_autocorr\nJohnson et al. (2016) estimated autocorrelation in a simulation context"
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-rockfish-steepness-profile",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-rockfish-steepness-profile",
    "title": "2  Modeling choices",
    "section": "2.8 Notes and best practices for rockfish steepness profile",
    "text": "2.8 Notes and best practices for rockfish steepness profile\nPacific rockfishes use a prior on steepness (or the predictive mean from this prior). A method was developed to develop this prior by Martin Dorn in 2007, and updated by Dorn in 2009/2011. It was then rebuilt by James Thorson, who updated it in 2013/2015/2017. Starting in 2019 the update will be conducted by Chantel Wetzel\n\nThe method is fully described in Thorson, Dorn, and Hamel (2018) Fisheries Research\nThe method can be replicated for any data set 2007/2009/2011/2013/2015/2017 using R package SebastesSteepness\nThe rationale for excluding a species when developing a prior for a given focal species (termed “Type-C” meta-analysis) is explained in this paper\n\nIf you are assessing any of the following species you will need to obtain a “Type-C” prior: aurora, black, bocaccio, canary, chilipepper, darkblotched, gopher, splitnose, widow, yelloweye, yellowtail north\n\nThe estimated prior by year is as follows:\n\n2007: mean = 0.58, sd = 0.181\n2009: mean = 0.69, sd = 0.218\n2011: mean = 0.76, sd = 0.170\n2013: mean = 0.78, sd = 0.152\n2015: mean = 0.77, sd = 0.147\n2017: mean = 0.72, sd = 0.158"
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-running-stock-synthesis",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-running-stock-synthesis",
    "title": "2  Notes and best practices",
    "section": "2.11 Notes and best practices for running Stock Synthesis",
    "text": "2.11 Notes and best practices for running Stock Synthesis\n\n2.11.1 Model setup\n\nBefore running anything, read your control file into Excel. It’s too hard to check the 14 columns if they aren’t lined up, and it’s easy to make mistakes in these values. Make sure the phases are small positive or negative integers (and not equal to 0), that the initial values are within the bounds, and nothing else looks strange.\nRun using only “ss.exe” at first, not “ss_opt.exe”. As longas you’re making changes to the model, you should be using this version which checks for array bound errors. If you get such an error, work to make it go away. Once the data are set and the parameters configured, you can switch to “SS3_opt.exe” and have the model run a little faster.\nRun the model without estimating anything at first. You should do this by setting “Turn off estimation for parameters entering after this phase” to 0 in the starter file or by adding “-maxfn 0” at the command line. If you don’t get a report file that looks OK at this stage, there’s no point in wasting time doing any estimation. Note: the “-noest” command line input does not seem to work as well as the starter file settings, so it’s probably best to avoid it.\nRead the “warning.sso” file after every run (or note the DOS index to check if warnings exist). If you don’t understand the warnings, find out, but don’t just ignore it.\nLook at “echoinput.sso” to debug models that don’t run. This file will show you what numbers ADMB has read and how they’re being interpreted by Stock Synthesis. Start at the bottom and scan upwards until things start to look right or start at the top and scan downwards until things start to look wrong. It’s often obvious when you have an extra input and things model starts to go awry. Use this information to fix your input files and try again.\nOnce the model runs, look at the “.ss_new” files. These files contain rich comments and often better formatting than your own input files. They are also good for debugging, because sometimes a model will run, but the parameter lines are associated with different fleets, or have different roles than you expected. Check the parameter names on the right hand side of “control.ss_new” to make sure everything looks right. You can then either replace your input files with the “.ss_new” files or just copy and paste elements that you want to keep. Note that if you’ve estimated any parameters, then the initial values in “control.ss_new” have been updated to these estimates.\nIf you have tagging data or time-varying catchability, utilize the option to automatically generate parameter lines related to these features using the “control.ss_new” file.\nPull in parameter bounds that are way too wide – if you aren’t anywhere near them during minimization, extremely wide bounds (like -15 to 15 on recruit deviations, or 1 to 35 for log-R0) just slow minimization and may result in poorer convergence properties.\nIf you have no initial equilibrium catch in the data file for a fleet, make sure the corresponding parameter is not being estimated in the control file. Note: the equilibrium setup is different in SS3 version 3.30, so if you have any equilibrium catch, you should learn about how this setup differs than in the past.\nFor indices of abundance, know what you’re doing. Don’t mix up a CV with a standard deviation of the log. If you’re using GLM output, make sure your year effects and uncertainty are transformed back into normal space (not log space or anything else).\n\n\n\n2.11.2 Model tuning\n\nConsider estimating an extra standard deviation parameter for indices. There are many reasons to expect that the input uncertainty values on indices of abundance are underestimates of the true uncertainty. Estimating an extra uncertainty parameter has worked well in a number of west coast groundfish assessments.\n\nNote that this should reflect the observed variability in survey indices rather than poor fit to the observed trend in survey indices. i.e. resist adding SD to surveys where there are trends in residuals without evidence of hyperdepletion or hyperstability, in which case a non-linear relationship between indices and stock size is more appropriate.\n\n\n\n\nWeight the composition data. Use one of the three methods described in the SS User Manual under “Data Weighting”. The McAllister-Ianelli method used to be the most common for west coast groundfish, but now Francis and Dirichlet-Multinomial are the dominant options, with no clear reason to choose one over the other. This might change by 2021.\nThe recruitment bias adjustment stuff is worth understanding. See Methot and Taylor (2011) or ask Ian to explain if this is confusing. The r4ss package has a function to estimate alternative inputs for the setup of recruitments in the control file. This can make a difference in model results.\nThink about sigmaR. This could be an arbitrarily chosen value, freely estimated, or iteratively tuned. Methot and Taylor (2011) suggest a way that the tuning could be done. Whatever you choose, put a little thought into it. SigmaR should be greater than the SD of the estimated recruitment deviations. See more detail on this above. There is a new (Dec. 2016) table called $sigma_R_info output by r4ss::SS_output() which provides information on tuning sigmaR.\n\n\n\n2.11.3 Estimation of uncertainty\n\nSee Stewart et al. (2012) or Methot and Taylor (2011) for some reasons why doing MCMC is a good thing if you have time for it.\nParametric bootstrap. This hasn’t been done much. The 2006 hake model included parametric bootstrap uncertainty estimates. A description is on page 41 and Figures 55-56 (pg 139-140) show some results. Doing N bootstraps requires\n\nrun model with N+2 for “Number of datafiles to produce” in starter.ss\nuse r4ss::SS_splitdat() or other tool to split apart data.ss_new\nrerun model with each new data file\nsummarize the results, perhaps using r4ss::SSsummarize()\n\n\n\n\nr4ss::SSplotPars() can be used to compare uncertainty estimated from MLE and MCMC. r4ss::SSplotComparisons() works as well.\n\n\n\n2.11.4 Using MCMC\n\nTalk to Ian Taylor Kelli Johnson or Aaron Berger if you want to run MCMC as they are using it with hake.\nThe burn-in and thinning options in the starter file are applied after whatever ADMB options are used at run time. That is, they are applied during the mceval step, not the mcmc step. You can also do burn-in or thinning in R.\nThe MCMC command automatically sets the recruitment bias adjustment settings to go to 1.0 for all years (see Methot & Taylor 2012 to see why). Therefore, the first sample of the MCMC chain probably won’t match your MLE.\nThe MCMC output is in the files posteriors.sso, derived_posteriors.sso and posterior_vectors.sso. Any Report associated with an MCMC run should not be used. It may reflect the bias adjustment settings for MCMC, not the MLE. Therefore, it’s safer to separate MCMC from MLE into different folders. To compare them using r4ss::SSplotPars(), you can merge the results into a shared folder.\nThe best available current approach to make plots from MCMC is through r4ss::SSplotComparisons(), which requires first reading MLE output and then adding the MCMC output. Talk to Ian or Allan about how to do this. Someday there will be a more generalized approach that can work directly with MCMC."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-retrospectives",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-retrospectives",
    "title": "2  Notes and best practices",
    "section": "2.12 Notes and best practices for retrospectives",
    "text": "2.12 Notes and best practices for retrospectives\n\nRetrospective analysis is a way to assess whether something is inconsistent in data or model assumptions. A retrospective pattern can arise from changes in catch, M, q, selectivity, or growth from that which is in the model. It is primarily an exploratory/diagnostic tool, though see point 4 below.\nSee the 11/17/2021 PEP team meeting presentation for background information and past publications on retrospective analysis\nPEP’s approach is to apply:\n\n5 years of peels (based on findings from Miller and Legault 2017),\nusing the alternative Mohn’s rho (Mohn’s rho averaged over peels - the AFSC_Hurtado term from r4ss::SSmohnsrho),\nfor depletion, biomass, fishing mortality, and recruitment. Often biomass, depletion, and recruitment are provided in assessment reports\n\n\n\n\nAlthough the east coast uses mohn’s rho to “correct” status indicators or when setting quotas, our practice is not to. This is based on past precedent, and that our fishing history is not as long as it is on the east coast.\nHurtado-Ferro et al. (2015) provide a rule of thumb on the significance of mohn’s rho (average over peels) dependent on life history. They suggest a retrospective pattern is not meaningful if \\(\\rho \\in (-0.15, 0.2)\\) for long lived species and \\(\\rho \\in (-0.22, 0.3)\\) for short lived species, and note that magnitude of Mohn’s rho not related to true bias in assessment. Miller and Legault (2017) argue that variance of Mohn’s rho is truly needed to ascertain whether an effect is significant."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-andres-rebuilder-program-aka-puntalyzer",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-andres-rebuilder-program-aka-puntalyzer",
    "title": "2  Notes and best practices",
    "section": "2.13 Notes and best practices for Andre’s rebuilder program (aka Puntalyzer)",
    "text": "2.13 Notes and best practices for Andre’s rebuilder program (aka Puntalyzer)\n\nBe very afraid\nTalk to Jason Cope, Vlada Gertseva, or Ian Taylor about what a nightmare the rebuilding analysis can be\nMake sure the “Do West Coast gfish rebuilder output (0/1)” is set to 1 in your SS3 forecast file. This will create the output files rebuild.dat and rebuild.sso\nDownload the rebuilder executable\nFigure out (from the manual perhaps) how to run the rebuilder and what modifications to your input files might be needed for what you need to do.\nNote that the rebuilder inputs have to be formatted more carefully than SS3 inputs. Blank lines or commented lines can’t be inserted without messing things up.\nRead the help file for r4ss::DoProjectPlots to learn about how to plot rebuilder output."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-r4ss",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-r4ss",
    "title": "2  Notes and best practices",
    "section": "2.14 Notes and best practices for r4ss",
    "text": "2.14 Notes and best practices for r4ss\n\nr4ss is available as an R package on CRAN but the most up-to-date version can be installed from GitHub. Installation instructions are available here.\nThe GitHub page linked above include a log of changes but it’s probably a good idea to reinstall it on a regular basis to keep up.\nLook through all the figures that get produced by SS_plots. Ian Stewart often said “each one of those figures is there for a reason”. There have been cases where models were put forth with incorrect assumptions about biology, ageing error, etc. that could have been caught if the assessment authors had paid attention to all the plots.\nRemember that the figures can be modified to look better. For instance, you can replace the fleet names in the model with a less abbreviated set of names that will go in the plot labels. Almost every aspect of these plots can be modified with inputs that don’t even require recoding R (although you can always do that too).\nIf something is not working right, complain about it on the r4ss issues list.\n\n\n2.14.1 Notes and best practices for writing assessment documents\n\nRead the terms of reference and actually follow them.\nConsider using LaTeX and .Rnw files. Chantel and Kelli are working on an R package that will serve as a template for all assessment documents and act as the successor to Melissa Monk’s template. based on R Markdown.\nIf you’re using Word, talk to power users of Word about formatting of section headings, links to figures or tables, etc.. These links save much time when document is updated with sections or elements added or removed.\n\n\n\nSee the Assessment Document Template for a pre-made setup that reflects 2017-18 TORs: \\\\nwcfile\\fram\\Assessments\\Archives and shared on the Google Drive\n\n\n\nUse proper dashes and hyphens for readability: Hyphen, En dash, and Em dash\nJason says: “It would be really easy to compare results of every assessment we do with the results of the data-poor methods.” He did this for Cabezon in 2009 and thinks we could do it for all our assessments.\nFor Word, consider using Endnote or Zotero to find/organize/insert references. The assessment endnote library is at \\\\Nwcfile\\fram\\Assessment\\References, but please export references you need and do not link a Word document directly to that library.\nMake sure that labels on Figures are readable. The best way to get figures produced in R into word is to save them as PNG files. The plotting functions in the R4SS package have an input for resolution that can be increased as needed. The default resolution is higher than what you get if you save the images one by one from the RGUI graphics window.\nCrop out non-informative headings and labels from figures imported from R4SS or other packages."
  },
  {
    "objectID": "02-notes-best-practices.html#stuff-to-do-well-in-advance-of-peak-stock-assessment-season",
    "href": "02-notes-best-practices.html#stuff-to-do-well-in-advance-of-peak-stock-assessment-season",
    "title": "2  Modeling choices",
    "section": "2.9 Stuff to do well in advance of peak stock assessment season",
    "text": "2.9 Stuff to do well in advance of peak stock assessment season\nContact Martin Dorn Jim Thorson Chantel Wetzel (ugh) for the current assessment cycle steepness prior. If you are conducting an assessment on a rockfish species that is included in the steepness profile (aurora, black, bocaccio, canary, chilipepper, darkblotched, gopher, splitnose, widow, yelloweye, yellowtail north), contact Chantel Wetzel for a “type-C” prior with your species removed."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-for-selecting-maximum-age-population-and-data",
    "href": "02-notes-best-practices.html#notes-and-best-practices-for-selecting-maximum-age-population-and-data",
    "title": "2  Modeling choices",
    "section": "2.1 Notes and best practices for selecting maximum age (population and data)",
    "text": "2.1 Notes and best practices for selecting maximum age (population and data)\nThe comments below are based on a team discussion on how to select a population and data maximum age for modeling:\nChoice 1, Population length bins: should extend beyond something like the 97.5% quantile of the length at age distribution for the largest age (easy to add more bins to see if it makes any difference).\nChoice 2, Max age: should be large enough for growth to be asymptotic, and at least as big as the largest data age bins. It’s easy to test the impact of changing to different value, just requires updating ageing error matrices. Look at the stable age-distribution plot and examine the number of fish in the plus group. Additionally, the period of time when data are available (after extensive exploitation), the data plus group may be lower.\nChoice 3, Data length bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. We should check to make sure that the selection of length and age bins are consistent with each other. Typically, we often have max length bins where there are only a few fish, but a larger proportion of data in the data age plus group.\nChoice 4, Data age bins should probably be big enough that the plus group doesn’t have much more in it than the previous bin. In regards to population maximum age, there is no negative repercussions within SS3 for having a larger value, beyond a slower run time. Revising the age bins based on the data, rather than apriori rules about how to set this, may be considered “data mining”. Could create a check within the survey and PacFIN comp codes that creates a flag when the data plus group has more than a certain percentage (i.e. 5%). Also, add a warning to r4ss about the percentage in the population and data plus groups."
  },
  {
    "objectID": "02-notes-best-practices.html#notes-and-best-practices-of-estimating-ageing-error",
    "href": "02-notes-best-practices.html#notes-and-best-practices-of-estimating-ageing-error",
    "title": "2  Modeling choices",
    "section": "2.3 Notes and best practices of estimating ageing error",
    "text": "2.3 Notes and best practices of estimating ageing error\nJim Thorson has been working on this. These files are accessible on the web.\nYou can also find the original user manual that goes with the age error ADMB executable written by Andre Punt that is called by the above wrapper on the google drive The above user manual will help you understand what the program is doing and how to setup and run an individual file if you wish to do so."
  },
  {
    "objectID": "05-other.html",
    "href": "05-other.html",
    "title": "5  Other useful tidbits",
    "section": "",
    "text": "Git is a version control system that can be used to synchronize files between the web and local directories. GitHub is a user-friendly website which hosts files and depends on Git.\n\n\n\nNEEDS UPDATE INCLUDING MORE INFO ABOUT WHAT EACH PACKAGE DOES– Update: Kristin Marshall has created a Google Sheet (on 16 March 2018) to track this documentation\nSome of the sets of code used in NWFSC assessments worth noting are the following:\n\nr4ss (package: https://github.com/r4ss/r4ss)\nnwfscMapping (package: https://github.com/nwfsc-assess/nwfscMapping)\nnwfscSurvey (package: https://github.com/nwfsc-assess/nwfscSurvey)\nVAST (package: https://github.com/James-Thorson/VAST)\nAdd https://github.com/James-Thorson/SebastesSteepness\nspatialDeltaGLMM (package: https://github.com/nwfsc-assess/geostatistical_delta-GLMM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscDeltaGLM (package: https://github.com/nwfsc-assess/nwfscDeltaGLM)\n\nSee GitHub URL readme for R package installation instructions and example\n\nnwfscAgeingError (package: https://github.com/nwfsc-assess/nwfscAgeingError)\n\nSee GitHub URL readme for R package installation instructions and example\n\nPlotting survey data with strata (from Jason):\nhttps://r-forge.r-project.org/scm/viewvc.php/JMC/?root=nwfscassmt\n[note: as of 10/28/16, plan hatched to move this file to GitHub soon]\nDiscard estimates with bootstrap uncertainty from WCGOP (https://github.com/nwfsc-assess/nwfscDiscardBootstrap)"
  },
  {
    "objectID": "05-other.html#stuff-to-do-well-in-advance-of-peak-stock-assessment-season",
    "href": "05-other.html#stuff-to-do-well-in-advance-of-peak-stock-assessment-season",
    "title": "5  Other useful tidbits",
    "section": "5.2 Stuff to do well in advance of peak stock assessment season",
    "text": "5.2 Stuff to do well in advance of peak stock assessment season\nContact Martin Dorn Jim Thorson Chantel Wetzel (ugh) for the current assessment cycle steepness prior. If you are conducting an assessment on a rockfish species that is included in the steepness profile (aurora, black, bocaccio, canary, chilipepper, darkblotched, gopher, splitnose, widow, yelloweye, yellowtail north), contact Chantel Wetzel for a “type-C” prior with your species removed."
  },
  {
    "objectID": "04-post-model.html",
    "href": "04-post-model.html",
    "title": "4  So you ran an assessment model",
    "section": "",
    "text": "Retrospective analysis is a way to assess whether something is inconsistent in data or model assumptions. A retrospective pattern can arise from changes in catch, M, q, selectivity, or growth from that which is in the model. It is primarily an exploratory/diagnostic tool, though see point 4 below.\nSee the 11/17/2021 PEP team meeting presentation for background information and past publications on retrospective analysis\nPEP’s approach is to apply:\n\n5 years of peels (based on findings from Miller and Legault 2017),\nusing the alternative Mohn’s rho (Mohn’s rho averaged over peels - the AFSC_Hurtado term from r4ss::SSmohnsrho),\nfor depletion, biomass, fishing mortality, and recruitment. Often biomass, depletion, and recruitment are provided in assessment reports\n\n\n\n\nAlthough the east coast uses mohn’s rho to “correct” status indicators or when setting quotas, our practice is not to. This is based on past precedent, and that our fishing history is not as long as it is on the east coast.\nHurtado-Ferro et al. (2015) provide a rule of thumb on the significance of mohn’s rho (average over peels) dependent on life history. They suggest a retrospective pattern is not meaningful if \\(\\rho \\in (-0.15, 0.2)\\) for long lived species and \\(\\rho \\in (-0.22, 0.3)\\) for short lived species, and note that magnitude of Mohn’s rho not related to true bias in assessment. Miller and Legault (2017) argue that variance of Mohn’s rho is truly needed to ascertain whether an effect is significant."
  },
  {
    "objectID": "04-post-model.html#notes-and-best-practices-for-andres-rebuilder-program-aka-puntalyzer",
    "href": "04-post-model.html#notes-and-best-practices-for-andres-rebuilder-program-aka-puntalyzer",
    "title": "4  So you ran an assessment model",
    "section": "4.2 Notes and best practices for Andre’s rebuilder program (aka Puntalyzer)",
    "text": "4.2 Notes and best practices for Andre’s rebuilder program (aka Puntalyzer)\n\nBe very afraid\nTalk to Jason Cope, Vlada Gertseva, or Ian Taylor about what a nightmare the rebuilding analysis can be\nMake sure the “Do West Coast gfish rebuilder output (0/1)” is set to 1 in your SS3 forecast file. This will create the output files rebuild.dat and rebuild.sso\nDownload the rebuilder executable\nFigure out (from the manual perhaps) how to run the rebuilder and what modifications to your input files might be needed for what you need to do.\nNote that the rebuilder inputs have to be formatted more carefully than SS3 inputs. Blank lines or commented lines can’t be inserted without messing things up.\nRead the help file for r4ss::DoProjectPlots to learn about how to plot rebuilder output."
  },
  {
    "objectID": "04-post-model.html#notes-and-best-practices-for-r4ss",
    "href": "04-post-model.html#notes-and-best-practices-for-r4ss",
    "title": "4  So you ran an assessment model",
    "section": "4.3 Notes and best practices for r4ss",
    "text": "4.3 Notes and best practices for r4ss\n\nr4ss is available as an R package on CRAN but the most up-to-date version can be installed from GitHub. Installation instructions are available here.\nThe GitHub page linked above include a log of changes but it’s probably a good idea to reinstall it on a regular basis to keep up.\nLook through all the figures that get produced by SS_plots. Ian Stewart often said “each one of those figures is there for a reason”. There have been cases where models were put forth with incorrect assumptions about biology, ageing error, etc. that could have been caught if the assessment authors had paid attention to all the plots.\nRemember that the figures can be modified to look better. For instance, you can replace the fleet names in the model with a less abbreviated set of names that will go in the plot labels. Almost every aspect of these plots can be modified with inputs that don’t even require recoding R (although you can always do that too).\nIf something is not working right, complain about it on the r4ss issues list.\n\n\n4.3.1 Notes and best practices for writing assessment documents\n\nRead the terms of reference and actually follow them.\nConsider using LaTeX and .Rnw files. Chantel and Kelli are working on an R package that will serve as a template for all assessment documents and act as the successor to Melissa Monk’s template. based on R Markdown.\nIf you’re using Word, talk to power users of Word about formatting of section headings, links to figures or tables, etc.. These links save much time when document is updated with sections or elements added or removed.\n\n\n\nSee the Assessment Document Template for a pre-made setup that reflects 2017-18 TORs: \\\\nwcfile\\fram\\Assessments\\Archives and shared on the Google Drive\n\n\n\nUse proper dashes and hyphens for readability: Hyphen, En dash, and Em dash\nJason says: “It would be really easy to compare results of every assessment we do with the results of the data-poor methods.” He did this for Cabezon in 2009 and thinks we could do it for all our assessments.\nFor Word, consider using Endnote or Zotero to find/organize/insert references. The assessment endnote library is at \\\\Nwcfile\\fram\\Assessment\\References, but please export references you need and do not link a Word document directly to that library.\nMake sure that labels on Figures are readable. The best way to get figures produced in R into word is to save them as PNG files. The plotting functions in the R4SS package have an input for resolution that can be increased as needed. The default resolution is higher than what you get if you save the images one by one from the RGUI graphics window.\nCrop out non-informative headings and labels from figures imported from R4SS or other packages."
  },
  {
    "objectID": "04-post-model.html#stuff-to-do-well-in-advance-of-peak-stock-assessment-season",
    "href": "04-post-model.html#stuff-to-do-well-in-advance-of-peak-stock-assessment-season",
    "title": "4  So you ran an assessment model",
    "section": "4.4 Stuff to do well in advance of peak stock assessment season",
    "text": "4.4 Stuff to do well in advance of peak stock assessment season\nContact Martin Dorn Jim Thorson Chantel Wetzel (ugh) for the current assessment cycle steepness prior. If you are conducting an assessment on a rockfish species that is included in the steepness profile (aurora, black, bocaccio, canary, chilipepper, darkblotched, gopher, splitnose, widow, yelloweye, yellowtail north), contact Chantel Wetzel for a “type-C” prior with your species removed."
  },
  {
    "objectID": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards",
    "href": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.6 Notes and best practices for observer data and discards",
    "text": "1.6 Notes and best practices for observer data and discards\nDiscard mortality should be accounted for within the assessment. There are typically three ways to approach this issue.\nThe first approach is to include discard mortality into the catch data rather than modeling. Discard mortality can be included by either adding this total to the catch by fleet or by adding a discard fleet. If using the discard fleet the selectivity of this fleet will need to be mirrored (see the 2017 California Blue/Deacon assessment for an example of this approach). To obtain estimates of total discards based on the fleet structure in the model (gear and/or area) speak with Kayleigh Somers (kayleigh.somers@noaa.gov). The annual groundfish mortality report will provide detailed information in the total estimate discard by species. The GEMM report (located on the network at \\Assessments\\GEMM Report) will provide a summarized version of the discard and retained estimates from the groundfish mortality report, but have information across all WCGOP years (2002 - present). However, the groundfish mortality report (or the GEMM) will not include the most full recent year estimates (example - the GEMM available in 2019 has data through 2017) which will require an assumption regarding the most recent year’s discards. Additionally, assumptions will need to be made regarding the total amount of discard prior to the start of the WCGOP data (pre-2002).\nThe second and third alternative approaches to including discard data is to model the process of discarding within SS3. Discards can be modeled either as total discards in mt or through the rate of discarding. Both of these approaches will require discard length data to be included in the model, or if not available a specific assumption regarding the retention curve relative to the estimated selectivity curve. Estimates of total discards are available in the annual groundfish mortality report (and the GEMM). If a specific breakdown of discard by fleet is required (gear and/or area) speak with Kayleigh Somers. The data may not be able to be summarized to all fleet structures, so it is best to discuss the options with Kayleigh early in the assessment process. Assumptions regarding the discard total in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of blocks). If total discards are modeled, one will need to enter an annual CV into SS. As of right now (2019), Kayleigh is not able to provide CV quantities. This information could be calculated based on the WCGOP based on the observed data using bootstrapping. However, the bootstrap CV will be based on the observer data and may not be consistent with the data available in the groundfish mortality report (and the GEMM). Please see below for additional information regarding bootstrapping.\nModeling discard rates can be an easier approach since it does not require knowledge about the total discard amounts. However, this approach may not be feasible for stocks with limited observations in the WCGOP data due to high variability in the observed discarded and retained fish. The IFQ trawl fleet currently has near 100% observer coverage, while the non-catch share sectors have a much lower observed percentage (~20% as of last inquiry, but this may change in the future). Historically, it was thought that discard rates could not be calculated for complex managed species. However, this is not the case based on the current data. Discard rates are calculated based on the observed discarded weight relative to the total observed weight of discarded and retained fish. Bootstrapping is done using the data in order to provide a CV to the discard rate. Similar to two previous approaches, assumptions regarding the discard rate in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of blocks)."
  },
  {
    "objectID": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards-1",
    "href": "01-data-sources.html#notes-and-best-practices-for-observer-data-and-discards-1",
    "title": "1  Data sources: notes, contact information, and links",
    "section": "1.7 Notes and best practices for observer data and discards",
    "text": "1.7 Notes and best practices for observer data and discards\nDiscard mortality should be accounted for within the assessment. There are typically three ways to approach this issue.\nThe first approach is to include discard mortality into the catch data rather than modeling. Discard mortality can be included by either adding this total to the catch by fleet or by adding a discard fleet. If using the discard fleet the selectivity of this fleet will need to be mirrored (see the 2017 California Blue/Deacon assessment for an example of this approach). To obtain estimates of total discards based on the fleet structure in the model (gear and/or area) speak with Kayleigh Somers (kayleigh.somers@noaa.gov). The annual groundfish mortality report will provide detailed information in the total estimate discard by species. The GEMM report (located on the network at \\Assessments\\GEMM Report) will provide a summarized version of the discard and retained estimates from the groundfish mortality report, but have information across all WCGOP years (2002 - present). However, the groundfish mortality report (or the GEMM) will not include the most full recent year estimates (example - the GEMM available in 2019 has data through 2017) which will require an assumption regarding the most recent year’s discards. Additionally, assumptions will need to be made regarding the total amount of discard prior to the start of the WCGOP data (pre-2002).\nThe second and third alternative approaches to including discard data is to model the process of discarding within SS3. Discards can be modeled either as total discards in mt or through the rate of discarding. Both of these approaches will require discard length data to be included in the model, or if not available a specific assumption regarding the retention curve relative to the estimated selectivity curve. Estimates of total discards are available in the annual groundfish mortality report (and the GEMM). If a specific breakdown of discard by fleet is required (gear and/or area) speak with Kayleigh Somers. The data may not be able to be summarized to all fleet structures, so it is best to discuss the options with Kayleigh early in the assessment process. Assumptions regarding the discard total in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of blocks). If total discards are modeled, one will need to enter an annual CV into SS. As of right now (2019), Kayleigh is not able to provide CV quantities. This information could be calculated based on the WCGOP based on the observed data using bootstrapping. However, the bootstrap CV will be based on the observer data and may not be consistent with the data available in the groundfish mortality report (and the GEMM). Please see below for additional information regarding bootstrapping.\nModeling discard rates can be an easier approach since it does not require knowledge about the total discard amounts. However, this approach may not be feasible for stocks with limited observations in the WCGOP data due to high variability in the observed discarded and retained fish. The IFQ trawl fleet currently has near 100% observer coverage, while the non-catch share sectors have a much lower observed percentage (~20% as of last inquiry, but this may change in the future). Historically, it was thought that discard rates could not be calculated for complex managed species. However, this is not the case based on the current data. Discard rates are calculated based on the observed discarded weight relative to the total observed weight of discarded and retained fish. Bootstrapping is done using the data in order to provide a CV to the discard rate. Similar to two previous approaches, assumptions regarding the discard rate in the most recent year and historical years prior to the start of WCGOP data (2002) will need to be made by the assessor and modeled (often through the use of blocks).\n\n1.7.1 What are the WCGOP data?\nThe WCGOP database includes only data collected by observers on West Coast vessels. The data available in this database varies fairly dramatically based on the sector. The IFQ vessels essentially have 100% observer coverage, so the database includes a comprehensive view of discards and retained catch from observers for that sector. However, other sectors have a much lower percentage of observer coverage (see the observer coverage). As an example, the nearshore fishery generally has about 20% observer coverage. Only the observations made on that percentage of vessels are going to be available in the WCGOP database. Hence, the observed retained and discard amounts are going to be a significant underestimate of total mortality in that sector. The discard ratio will also be more variable, but could generally be representative of the discarding behavior.\n\n\n1.7.2 Bootstrapping\nThe current approach to obtain uncertainty around the data in the WCGOP database, either the total discard or discard rates, is to bootstrap the data. These data are summarized and the observations bootstrapped to obtain uncertainty estimated based on gear and area stratification requested by the assessor. Chantel Wetzel (chantel.wetzel@noaa.gov) currently conducts the bootstrap analysis.\n\n\n1.7.3 Requesting discard data\nThere are two types of discard data available using WCGOP data. The first is a summary of the observed discards, discard rates, and bootstrapped uncertainty across years (2002 - present). The second type of data are lengths of discarded fish observed by WCGOP. To request these data please email information regarding data stratification (gear groupings and areas) to Chantel Wetzel (chantel.wetzel@noaa.gov) for observed discards, rates, and uncertainty and Andi Stephens (andi.stephens@noaa.gov) for length composition data for discarded fish.\n\n\n1.7.4 Discard rates, and length comps, from the Pikitch et al. Discard Study (1985-87) and Mesh Study (1988-90) databases\nIf enough data exists, discard rates and length comps from the Pikitch et al. Discard Study database (or Mesh Study if no data exists in the Discard Study) may be obtained for a species to assessed from John Wallace. Optionally, these rates can be expanded out to the surrounding years based applying the study rates to PacFIN catch from the expanded years. This, of course, only works when the assumption of no significant changes to the fishery is a good one. A draft of the Word document of the methods paper (with sablefish example results soon to be updated) can be downloaded here."
  }
]